{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna-integration[tfkeras] in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (4.2.1)\n",
      "Requirement already satisfied: optuna in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from optuna-integration[tfkeras]) (4.2.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from optuna-integration[tfkeras]) (2.18.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from optuna->optuna-integration[tfkeras]) (1.14.1)\n",
      "Requirement already satisfied: colorlog in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from optuna->optuna-integration[tfkeras]) (6.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from optuna->optuna-integration[tfkeras]) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from optuna->optuna-integration[tfkeras]) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from optuna->optuna-integration[tfkeras]) (2.0.37)\n",
      "Requirement already satisfied: tqdm in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from optuna->optuna-integration[tfkeras]) (4.66.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from optuna->optuna-integration[tfkeras]) (6.0.1)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow->optuna-integration[tfkeras]) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (3.8.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (0.31.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from alembic>=1.5.0->optuna->optuna-integration[tfkeras]) (1.3.8)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[tfkeras]) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from colorlog->optuna->optuna-integration[tfkeras]) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (0.41.3)\n",
      "Requirement already satisfied: rich in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration[tfkeras]) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\79112\\.conda\\envs\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow->optuna-integration[tfkeras]) (0.1.2)\n",
      "Collecting pytorch\n",
      "  Using cached pytorch-1.0.2.tar.gz (689 bytes)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: pytorch\n",
      "  Building wheel for pytorch (setup.py): started\n",
      "  Building wheel for pytorch (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for pytorch\n",
      "Failed to build pytorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [6 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\79112\\AppData\\Local\\Temp\\pip-install-lpx99y62\\pytorch_af256cc7e154476c82043e098369c0e2\\setup.py\", line 15, in <module>\n",
      "          raise Exception(message)\n",
      "      Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for pytorch\n",
      "ERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install -U optuna-integration[tfkeras]\n",
    "!pip install -U pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from feature_engineering import AAVE, ARB, BNB, BTC, ETH, JASMY, LDO, LINK, OP, PENDLE\n",
    "from data_preprocessing import prepare_data, prepare_for_train\n",
    "from vizualize import show_example\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, concatenate, Lambda, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore', Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>SMA_delta</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>BB_delta_cu</th>\n",
       "      <th>BB_delta_cl</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Revers</th>\n",
       "      <th>Moment</th>\n",
       "      <th>next_high</th>\n",
       "      <th>next_low</th>\n",
       "      <th>next_close</th>\n",
       "      <th>dynamic_range</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2023-01-02 22:45:00</td>\n",
       "      <td>53.50</td>\n",
       "      <td>53.50</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.20</td>\n",
       "      <td>-1.20850</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>0.705370</td>\n",
       "      <td>-0.164630</td>\n",
       "      <td>0.224202</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.003731</td>\n",
       "      <td>53.50</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.30</td>\n",
       "      <td>0.004214</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2023-01-02 23:00:00</td>\n",
       "      <td>53.30</td>\n",
       "      <td>53.50</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.30</td>\n",
       "      <td>-1.19000</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>0.627246</td>\n",
       "      <td>-0.012754</td>\n",
       "      <td>0.234309</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>-0.005623</td>\n",
       "      <td>53.40</td>\n",
       "      <td>53.30</td>\n",
       "      <td>53.30</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2023-01-02 23:15:00</td>\n",
       "      <td>53.30</td>\n",
       "      <td>53.40</td>\n",
       "      <td>53.30</td>\n",
       "      <td>53.30</td>\n",
       "      <td>-1.17150</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>0.642491</td>\n",
       "      <td>0.032491</td>\n",
       "      <td>0.229734</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.40</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.40</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2023-01-02 23:30:00</td>\n",
       "      <td>53.30</td>\n",
       "      <td>53.40</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.40</td>\n",
       "      <td>-1.15700</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>0.544142</td>\n",
       "      <td>0.154142</td>\n",
       "      <td>0.212436</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.40</td>\n",
       "      <td>52.90</td>\n",
       "      <td>53.00</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>2023-01-02 23:45:00</td>\n",
       "      <td>53.30</td>\n",
       "      <td>53.40</td>\n",
       "      <td>52.90</td>\n",
       "      <td>53.00</td>\n",
       "      <td>-1.12900</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>1.004051</td>\n",
       "      <td>-0.135949</td>\n",
       "      <td>0.210778</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>53.10</td>\n",
       "      <td>52.90</td>\n",
       "      <td>53.00</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34945</th>\n",
       "      <td>2023-12-30 22:30:00</td>\n",
       "      <td>112.08</td>\n",
       "      <td>112.52</td>\n",
       "      <td>111.85</td>\n",
       "      <td>112.41</td>\n",
       "      <td>1.04415</td>\n",
       "      <td>49.453552</td>\n",
       "      <td>0.521033</td>\n",
       "      <td>0.893033</td>\n",
       "      <td>0.726014</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.001516</td>\n",
       "      <td>112.48</td>\n",
       "      <td>111.33</td>\n",
       "      <td>112.09</td>\n",
       "      <td>0.006459</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34946</th>\n",
       "      <td>2023-12-30 22:45:00</td>\n",
       "      <td>112.43</td>\n",
       "      <td>112.48</td>\n",
       "      <td>111.33</td>\n",
       "      <td>112.09</td>\n",
       "      <td>1.00110</td>\n",
       "      <td>44.783715</td>\n",
       "      <td>0.830014</td>\n",
       "      <td>0.531014</td>\n",
       "      <td>0.713212</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>112.95</td>\n",
       "      <td>112.02</td>\n",
       "      <td>112.45</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34947</th>\n",
       "      <td>2023-12-30 23:00:00</td>\n",
       "      <td>112.08</td>\n",
       "      <td>112.95</td>\n",
       "      <td>112.02</td>\n",
       "      <td>112.45</td>\n",
       "      <td>0.92620</td>\n",
       "      <td>52.736318</td>\n",
       "      <td>0.450825</td>\n",
       "      <td>0.786825</td>\n",
       "      <td>0.834117</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>-0.003029</td>\n",
       "      <td>112.51</td>\n",
       "      <td>112.03</td>\n",
       "      <td>112.15</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34948</th>\n",
       "      <td>2023-12-30 23:15:00</td>\n",
       "      <td>112.40</td>\n",
       "      <td>112.51</td>\n",
       "      <td>112.03</td>\n",
       "      <td>112.15</td>\n",
       "      <td>0.85815</td>\n",
       "      <td>53.132832</td>\n",
       "      <td>0.676968</td>\n",
       "      <td>0.348968</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>112.33</td>\n",
       "      <td>111.49</td>\n",
       "      <td>111.50</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34949</th>\n",
       "      <td>2023-12-30 23:30:00</td>\n",
       "      <td>112.10</td>\n",
       "      <td>112.33</td>\n",
       "      <td>111.49</td>\n",
       "      <td>111.50</td>\n",
       "      <td>0.83045</td>\n",
       "      <td>39.568345</td>\n",
       "      <td>1.364445</td>\n",
       "      <td>-0.246555</td>\n",
       "      <td>0.753804</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>-0.002227</td>\n",
       "      <td>111.66</td>\n",
       "      <td>110.90</td>\n",
       "      <td>111.02</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34751 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                timestamp    open    high     low   close  SMA_delta  \\\n",
       "199   2023-01-02 22:45:00   53.50   53.50   53.20   53.20   -1.20850   \n",
       "200   2023-01-02 23:00:00   53.30   53.50   53.20   53.30   -1.19000   \n",
       "201   2023-01-02 23:15:00   53.30   53.40   53.30   53.30   -1.17150   \n",
       "202   2023-01-02 23:30:00   53.30   53.40   53.20   53.40   -1.15700   \n",
       "203   2023-01-02 23:45:00   53.30   53.40   52.90   53.00   -1.12900   \n",
       "...                   ...     ...     ...     ...     ...        ...   \n",
       "34945 2023-12-30 22:30:00  112.08  112.52  111.85  112.41    1.04415   \n",
       "34946 2023-12-30 22:45:00  112.43  112.48  111.33  112.09    1.00110   \n",
       "34947 2023-12-30 23:00:00  112.08  112.95  112.02  112.45    0.92620   \n",
       "34948 2023-12-30 23:15:00  112.40  112.51  112.03  112.15    0.85815   \n",
       "34949 2023-12-30 23:30:00  112.10  112.33  111.49  111.50    0.83045   \n",
       "\n",
       "          RSI_14  BB_delta_cu  BB_delta_cl       ATR    Revers    Moment  \\\n",
       "199    30.769231     0.705370    -0.164630  0.224202 -0.000000 -0.003731   \n",
       "200    35.714286     0.627246    -0.012754  0.234309 -0.001878 -0.005623   \n",
       "201    35.714286     0.642491     0.032491  0.229734 -0.000000  0.000000   \n",
       "202    42.857143     0.544142     0.154142  0.212436 -0.000000  0.000000   \n",
       "203    29.411765     1.004051    -0.135949  0.210778  0.001874  0.001874   \n",
       "...          ...          ...          ...       ...       ...       ...   \n",
       "34945  49.453552     0.521033     0.893033  0.726014 -0.000089 -0.001516   \n",
       "34946  44.783715     0.830014     0.531014  0.713212 -0.000178  0.002940   \n",
       "34947  52.736318     0.450825     0.786825  0.834117  0.000089 -0.003029   \n",
       "34948  53.132832     0.676968     0.348968  0.788235  0.000445  0.003296   \n",
       "34949  39.568345     1.364445    -0.246555  0.753804  0.000446 -0.002227   \n",
       "\n",
       "       next_high  next_low  next_close  dynamic_range  target  \n",
       "199        53.50     53.20       53.30       0.004214       1  \n",
       "200        53.40     53.30       53.30       0.004396       0  \n",
       "201        53.40     53.20       53.40       0.004310       0  \n",
       "202        53.40     52.90       53.00       0.003978      -1  \n",
       "203        53.10     52.90       53.00       0.003977       0  \n",
       "...          ...       ...         ...            ...     ...  \n",
       "34945     112.48    111.33      112.09       0.006459      -1  \n",
       "34946     112.95    112.02      112.45       0.006363       1  \n",
       "34947     112.51    112.03      112.15       0.007418       0  \n",
       "34948     112.33    111.49      111.50       0.007028       0  \n",
       "34949     111.66    110.90      111.02       0.006761       0  \n",
       "\n",
       "[34751 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prepare_data(AAVE, multiplier=2).copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "close": [
          53.2,
          53.3,
          53.3,
          53.4,
          53,
          53,
          52.9,
          53.1,
          53.2,
          53,
          52.7,
          52.9,
          52.9,
          53,
          52.8,
          52.9,
          52.8,
          52.7,
          52.9,
          52.9,
          53,
          53,
          53,
          52.9,
          52.9,
          52.9,
          52.9,
          53.1,
          53.2,
          53.2,
          53.3,
          53.1,
          53,
          53,
          53,
          53.1,
          53.1,
          53,
          52.8,
          52.7,
          52.9,
          52.9,
          53,
          53.1,
          53.1,
          53.1,
          53.1,
          53.1,
          53,
          53
         ],
         "high": [
          53.5,
          53.5,
          53.4,
          53.4,
          53.4,
          53.1,
          53.1,
          53.2,
          53.3,
          53.2,
          53.1,
          53,
          53,
          53,
          53,
          53,
          52.9,
          52.9,
          53,
          53.1,
          53.1,
          53.1,
          53.1,
          53.1,
          53,
          53,
          53.1,
          53.2,
          53.3,
          53.4,
          53.4,
          53.4,
          53.2,
          53.1,
          53.1,
          53.1,
          53.2,
          53.2,
          53.1,
          52.9,
          53,
          52.9,
          53.1,
          53.2,
          53.3,
          53.3,
          53.2,
          53.2,
          53.2,
          53.1
         ],
         "low": [
          53.2,
          53.2,
          53.3,
          53.2,
          52.9,
          52.9,
          52.9,
          52.9,
          53.1,
          53,
          52.6,
          52.6,
          52.8,
          52.8,
          52.8,
          52.8,
          52.8,
          52.6,
          52.7,
          52.8,
          52.9,
          53,
          52.9,
          52.8,
          52.8,
          52.9,
          52.9,
          52.9,
          53.1,
          53.1,
          53.1,
          53.1,
          53,
          52.9,
          52.9,
          53,
          53.1,
          53,
          52.8,
          52.7,
          52.7,
          52.8,
          52.8,
          53,
          53.1,
          53.1,
          53.1,
          53,
          53,
          53
         ],
         "name": "Candlestick",
         "open": [
          53.5,
          53.3,
          53.3,
          53.3,
          53.3,
          52.9,
          53,
          52.9,
          53.1,
          53.1,
          53,
          52.7,
          53,
          52.9,
          53,
          52.8,
          52.9,
          52.8,
          52.7,
          52.8,
          52.9,
          53,
          53,
          53,
          52.9,
          52.9,
          53,
          52.9,
          53.1,
          53.2,
          53.2,
          53.3,
          53.1,
          53,
          53,
          53,
          53.1,
          53.1,
          53,
          52.9,
          52.7,
          52.9,
          52.8,
          53,
          53.1,
          53.2,
          53.2,
          53.1,
          53.1,
          53
         ],
         "type": "candlestick",
         "x": [
          "2023-01-02T22:45:00",
          "2023-01-02T23:00:00",
          "2023-01-02T23:15:00",
          "2023-01-02T23:30:00",
          "2023-01-02T23:45:00",
          "2023-01-03T00:00:00",
          "2023-01-03T00:15:00",
          "2023-01-03T00:30:00",
          "2023-01-03T00:45:00",
          "2023-01-03T01:00:00",
          "2023-01-03T01:15:00",
          "2023-01-03T01:30:00",
          "2023-01-03T01:45:00",
          "2023-01-03T02:00:00",
          "2023-01-03T02:15:00",
          "2023-01-03T02:30:00",
          "2023-01-03T02:45:00",
          "2023-01-03T03:00:00",
          "2023-01-03T03:15:00",
          "2023-01-03T03:30:00",
          "2023-01-03T03:45:00",
          "2023-01-03T04:00:00",
          "2023-01-03T04:15:00",
          "2023-01-03T04:30:00",
          "2023-01-03T04:45:00",
          "2023-01-03T05:00:00",
          "2023-01-03T05:15:00",
          "2023-01-03T05:30:00",
          "2023-01-03T05:45:00",
          "2023-01-03T06:00:00",
          "2023-01-03T06:15:00",
          "2023-01-03T06:30:00",
          "2023-01-03T06:45:00",
          "2023-01-03T07:00:00",
          "2023-01-03T07:15:00",
          "2023-01-03T07:30:00",
          "2023-01-03T07:45:00",
          "2023-01-03T08:00:00",
          "2023-01-03T08:15:00",
          "2023-01-03T08:30:00",
          "2023-01-03T08:45:00",
          "2023-01-03T09:00:00",
          "2023-01-03T09:15:00",
          "2023-01-03T09:30:00",
          "2023-01-03T09:45:00",
          "2023-01-03T10:00:00",
          "2023-01-03T10:15:00",
          "2023-01-03T10:30:00",
          "2023-01-03T10:45:00",
          "2023-01-03T11:00:00"
         ]
        },
        {
         "marker": {
          "color": "green",
          "size": 8,
          "symbol": "triangle-up"
         },
         "mode": "markers+text",
         "name": "Long TP",
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-02T23:00:00",
          "2023-01-03T00:30:00",
          "2023-01-03T01:30:00",
          "2023-01-03T03:15:00",
          "2023-01-03T05:30:00",
          "2023-01-03T08:45:00"
         ],
         "y": [
          53.4242023433022,
          53.10809557758878,
          52.93505417859779,
          52.91232346379393,
          53.11980820216693,
          52.88821643167479
         ]
        },
        {
         "marker": {
          "color": "red",
          "size": 8,
          "symbol": "triangle-down"
         },
         "mode": "markers+text",
         "name": "Long SL",
         "textposition": "bottom center",
         "type": "scatter",
         "x": [
          "2023-01-02T23:00:00",
          "2023-01-03T00:30:00",
          "2023-01-03T01:30:00",
          "2023-01-03T03:15:00",
          "2023-01-03T05:30:00",
          "2023-01-03T08:45:00"
         ],
         "y": [
          52.97579765669781,
          52.69190442241123,
          52.46494582140222,
          52.48767653620608,
          52.680191797833075,
          52.51178356832522
         ]
        },
        {
         "marker": {
          "color": "green",
          "size": 8,
          "symbol": "triangle-down"
         },
         "mode": "markers+text",
         "name": "Short TP",
         "textposition": "bottom center",
         "type": "scatter",
         "x": [
          "2023-01-02T23:45:00",
          "2023-01-03T01:15:00"
         ],
         "y": [
          53.18756368941188,
          52.77493748623332
         ]
        },
        {
         "marker": {
          "color": "red",
          "size": 8,
          "symbol": "triangle-up"
         },
         "mode": "markers+text",
         "name": "Short SL",
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-02T23:45:00",
          "2023-01-03T01:15:00"
         ],
         "y": [
          53.61243631058812,
          53.22506251376668
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Buy",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-02T22:45:00"
         ],
         "y": [
          53.2
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-02T23:00:00"
         ],
         "y": [
          53.3
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-02T23:15:00"
         ],
         "y": [
          53.3
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Sell",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-02T23:30:00"
         ],
         "y": [
          53.4
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-02T23:45:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T00:00:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Buy",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T00:15:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T00:30:00"
         ],
         "y": [
          53.1
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T00:45:00"
         ],
         "y": [
          53.2
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Sell",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T01:00:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Buy",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T01:15:00"
         ],
         "y": [
          52.7
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T01:30:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T01:45:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T02:00:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T02:15:00"
         ],
         "y": [
          52.8
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T02:30:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T02:45:00"
         ],
         "y": [
          52.8
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Buy",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T03:00:00"
         ],
         "y": [
          52.7
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T03:15:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T03:30:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T03:45:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T04:00:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T04:15:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T04:30:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T04:45:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T05:00:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Buy",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T05:15:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T05:30:00"
         ],
         "y": [
          53.1
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T05:45:00"
         ],
         "y": [
          53.2
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T06:00:00"
         ],
         "y": [
          53.2
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T06:15:00"
         ],
         "y": [
          53.3
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T06:30:00"
         ],
         "y": [
          53.1
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T06:45:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T07:00:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T07:15:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T07:30:00"
         ],
         "y": [
          53.1
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T07:45:00"
         ],
         "y": [
          53.1
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T08:00:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T08:15:00"
         ],
         "y": [
          52.8
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Buy",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T08:30:00"
         ],
         "y": [
          52.7
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T08:45:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T09:00:00"
         ],
         "y": [
          52.9
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T09:15:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T09:30:00"
         ],
         "y": [
          53.1
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T09:45:00"
         ],
         "y": [
          53.1
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T10:00:00"
         ],
         "y": [
          53.1
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T10:15:00"
         ],
         "y": [
          53.1
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T10:30:00"
         ],
         "y": [
          53.1
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T10:45:00"
         ],
         "y": [
          53
         ]
        },
        {
         "mode": "text",
         "showlegend": false,
         "text": "Stay",
         "textfont": {
          "size": 14
         },
         "textposition": "top center",
         "type": "scatter",
         "x": [
          "2023-01-03T11:00:00"
         ],
         "y": [
          53
         ]
        },
        {
         "line": {
          "color": "green",
          "width": 1.5
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-02T22:45:00",
          "2023-01-02T23:00:00"
         ],
         "y": [
          53.2,
          53.4242023433022
         ]
        },
        {
         "marker": {
          "color": "green",
          "size": 16,
          "symbol": "triangle-up"
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-02T23:00:00"
         ],
         "y": [
          53.4242023433022
         ]
        },
        {
         "line": {
          "color": "green",
          "width": 1.5
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-02T23:30:00",
          "2023-01-02T23:45:00"
         ],
         "y": [
          53.4,
          53.18756368941188
         ]
        },
        {
         "marker": {
          "color": "green",
          "size": 16,
          "symbol": "triangle-down"
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-02T23:45:00"
         ],
         "y": [
          53.18756368941188
         ]
        },
        {
         "line": {
          "color": "green",
          "width": 1.5
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T00:15:00",
          "2023-01-03T00:30:00"
         ],
         "y": [
          52.9,
          53.10809557758878
         ]
        },
        {
         "marker": {
          "color": "green",
          "size": 16,
          "symbol": "triangle-up"
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T00:30:00"
         ],
         "y": [
          53.10809557758878
         ]
        },
        {
         "line": {
          "color": "green",
          "width": 1.5
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T01:00:00",
          "2023-01-03T01:15:00"
         ],
         "y": [
          53,
          52.77493748623332
         ]
        },
        {
         "marker": {
          "color": "green",
          "size": 16,
          "symbol": "triangle-down"
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T01:15:00"
         ],
         "y": [
          52.77493748623332
         ]
        },
        {
         "line": {
          "color": "green",
          "width": 1.5
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T01:15:00",
          "2023-01-03T01:30:00"
         ],
         "y": [
          52.7,
          52.93505417859779
         ]
        },
        {
         "marker": {
          "color": "green",
          "size": 16,
          "symbol": "triangle-up"
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T01:30:00"
         ],
         "y": [
          52.93505417859779
         ]
        },
        {
         "line": {
          "color": "green",
          "width": 1.5
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T03:00:00",
          "2023-01-03T03:15:00"
         ],
         "y": [
          52.7,
          52.91232346379393
         ]
        },
        {
         "marker": {
          "color": "green",
          "size": 16,
          "symbol": "triangle-up"
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T03:15:00"
         ],
         "y": [
          52.91232346379393
         ]
        },
        {
         "line": {
          "color": "green",
          "width": 1.5
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T05:15:00",
          "2023-01-03T05:30:00"
         ],
         "y": [
          52.9,
          53.11980820216693
         ]
        },
        {
         "marker": {
          "color": "green",
          "size": 16,
          "symbol": "triangle-up"
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T05:30:00"
         ],
         "y": [
          53.11980820216693
         ]
        },
        {
         "line": {
          "color": "green",
          "width": 1.5
         },
         "marker": {
          "size": 6
         },
         "mode": "lines+markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T08:30:00",
          "2023-01-03T08:45:00"
         ],
         "y": [
          52.7,
          52.88821643167479
         ]
        },
        {
         "marker": {
          "color": "green",
          "size": 16,
          "symbol": "triangle-up"
         },
         "mode": "markers",
         "showlegend": false,
         "type": "scatter",
         "x": [
          "2023-01-03T08:45:00"
         ],
         "y": [
          52.88821643167479
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "color": "green",
           "size": 12
          },
          "showarrow": false,
          "text": "0.42%",
          "x": "2023-01-02T23:00:00",
          "xshift": -20,
          "y": 53.4242023433022,
          "yshift": 10
         },
         {
          "font": {
           "color": "green",
           "size": 12
          },
          "showarrow": false,
          "text": "0.40%",
          "x": "2023-01-02T23:45:00",
          "xshift": -20,
          "y": 53.18756368941188,
          "yshift": -10
         },
         {
          "font": {
           "color": "green",
           "size": 12
          },
          "showarrow": false,
          "text": "0.39%",
          "x": "2023-01-03T00:30:00",
          "xshift": -20,
          "y": 53.10809557758878,
          "yshift": 10
         },
         {
          "font": {
           "color": "green",
           "size": 12
          },
          "showarrow": false,
          "text": "0.42%",
          "x": "2023-01-03T01:15:00",
          "xshift": -20,
          "y": 52.77493748623332,
          "yshift": -10
         },
         {
          "font": {
           "color": "green",
           "size": 12
          },
          "showarrow": false,
          "text": "0.45%",
          "x": "2023-01-03T01:30:00",
          "xshift": -20,
          "y": 52.93505417859779,
          "yshift": 10
         },
         {
          "font": {
           "color": "green",
           "size": 12
          },
          "showarrow": false,
          "text": "0.40%",
          "x": "2023-01-03T03:15:00",
          "xshift": -20,
          "y": 52.91232346379393,
          "yshift": 10
         },
         {
          "font": {
           "color": "green",
           "size": 12
          },
          "showarrow": false,
          "text": "0.42%",
          "x": "2023-01-03T05:30:00",
          "xshift": -20,
          "y": 53.11980820216693,
          "yshift": 10
         },
         {
          "font": {
           "color": "green",
           "size": 12
          },
          "showarrow": false,
          "text": "0.36%",
          "x": "2023-01-03T08:45:00",
          "xshift": -20,
          "y": 52.88821643167479,
          "yshift": 10
         }
        ],
        "height": 800,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Свечной график"
        },
        "width": 1500,
        "xaxis": {
         "rangeslider": {
          "visible": false
         },
         "title": {
          "text": "Время"
         }
        },
        "yaxis": {
         "title": {
          "text": "Цена"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_example(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>SMA_delta</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>BB_delta_cu</th>\n",
       "      <th>BB_delta_cl</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Revers</th>\n",
       "      <th>Moment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53.50</td>\n",
       "      <td>53.50</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.20</td>\n",
       "      <td>-1.20850</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>0.705370</td>\n",
       "      <td>-0.164630</td>\n",
       "      <td>0.224202</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.003731</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.30</td>\n",
       "      <td>53.50</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.30</td>\n",
       "      <td>-1.19000</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>0.627246</td>\n",
       "      <td>-0.012754</td>\n",
       "      <td>0.234309</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>-0.005623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53.30</td>\n",
       "      <td>53.40</td>\n",
       "      <td>53.30</td>\n",
       "      <td>53.30</td>\n",
       "      <td>-1.17150</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>0.642491</td>\n",
       "      <td>0.032491</td>\n",
       "      <td>0.229734</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.30</td>\n",
       "      <td>53.40</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.40</td>\n",
       "      <td>-1.15700</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>0.544142</td>\n",
       "      <td>0.154142</td>\n",
       "      <td>0.212436</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53.30</td>\n",
       "      <td>53.40</td>\n",
       "      <td>52.90</td>\n",
       "      <td>53.00</td>\n",
       "      <td>-1.12900</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>1.004051</td>\n",
       "      <td>-0.135949</td>\n",
       "      <td>0.210778</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34746</th>\n",
       "      <td>112.08</td>\n",
       "      <td>112.52</td>\n",
       "      <td>111.85</td>\n",
       "      <td>112.41</td>\n",
       "      <td>1.04415</td>\n",
       "      <td>49.453552</td>\n",
       "      <td>0.521033</td>\n",
       "      <td>0.893033</td>\n",
       "      <td>0.726014</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.001516</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34747</th>\n",
       "      <td>112.43</td>\n",
       "      <td>112.48</td>\n",
       "      <td>111.33</td>\n",
       "      <td>112.09</td>\n",
       "      <td>1.00110</td>\n",
       "      <td>44.783715</td>\n",
       "      <td>0.830014</td>\n",
       "      <td>0.531014</td>\n",
       "      <td>0.713212</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34748</th>\n",
       "      <td>112.08</td>\n",
       "      <td>112.95</td>\n",
       "      <td>112.02</td>\n",
       "      <td>112.45</td>\n",
       "      <td>0.92620</td>\n",
       "      <td>52.736318</td>\n",
       "      <td>0.450825</td>\n",
       "      <td>0.786825</td>\n",
       "      <td>0.834117</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>-0.003029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34749</th>\n",
       "      <td>112.40</td>\n",
       "      <td>112.51</td>\n",
       "      <td>112.03</td>\n",
       "      <td>112.15</td>\n",
       "      <td>0.85815</td>\n",
       "      <td>53.132832</td>\n",
       "      <td>0.676968</td>\n",
       "      <td>0.348968</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34750</th>\n",
       "      <td>112.10</td>\n",
       "      <td>112.33</td>\n",
       "      <td>111.49</td>\n",
       "      <td>111.50</td>\n",
       "      <td>0.83045</td>\n",
       "      <td>39.568345</td>\n",
       "      <td>1.364445</td>\n",
       "      <td>-0.246555</td>\n",
       "      <td>0.753804</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>-0.002227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34751 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         open    high     low   close  SMA_delta     RSI_14  BB_delta_cu  \\\n",
       "0       53.50   53.50   53.20   53.20   -1.20850  30.769231     0.705370   \n",
       "1       53.30   53.50   53.20   53.30   -1.19000  35.714286     0.627246   \n",
       "2       53.30   53.40   53.30   53.30   -1.17150  35.714286     0.642491   \n",
       "3       53.30   53.40   53.20   53.40   -1.15700  42.857143     0.544142   \n",
       "4       53.30   53.40   52.90   53.00   -1.12900  29.411765     1.004051   \n",
       "...       ...     ...     ...     ...        ...        ...          ...   \n",
       "34746  112.08  112.52  111.85  112.41    1.04415  49.453552     0.521033   \n",
       "34747  112.43  112.48  111.33  112.09    1.00110  44.783715     0.830014   \n",
       "34748  112.08  112.95  112.02  112.45    0.92620  52.736318     0.450825   \n",
       "34749  112.40  112.51  112.03  112.15    0.85815  53.132832     0.676968   \n",
       "34750  112.10  112.33  111.49  111.50    0.83045  39.568345     1.364445   \n",
       "\n",
       "       BB_delta_cl       ATR    Revers    Moment  target  \n",
       "0        -0.164630  0.224202 -0.000000 -0.003731       1  \n",
       "1        -0.012754  0.234309 -0.001878 -0.005623       0  \n",
       "2         0.032491  0.229734 -0.000000  0.000000       0  \n",
       "3         0.154142  0.212436 -0.000000  0.000000      -1  \n",
       "4        -0.135949  0.210778  0.001874  0.001874       0  \n",
       "...            ...       ...       ...       ...     ...  \n",
       "34746     0.893033  0.726014 -0.000089 -0.001516      -1  \n",
       "34747     0.531014  0.713212 -0.000178  0.002940       1  \n",
       "34748     0.786825  0.834117  0.000089 -0.003029       0  \n",
       "34749     0.348968  0.788235  0.000445  0.003296       0  \n",
       "34750    -0.246555  0.753804  0.000446 -0.002227       0  \n",
       "\n",
       "[34751 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_for_train(df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('timestamp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['target'].replace({-1: 1, 1: 2}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>SMA_delta</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>BB_delta_cu</th>\n",
       "      <th>BB_delta_cl</th>\n",
       "      <th>ATR</th>\n",
       "      <th>Revers</th>\n",
       "      <th>Moment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>53.50</td>\n",
       "      <td>53.50</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.20</td>\n",
       "      <td>-1.20850</td>\n",
       "      <td>30.769231</td>\n",
       "      <td>0.705370</td>\n",
       "      <td>-0.164630</td>\n",
       "      <td>0.224202</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.003731</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>53.30</td>\n",
       "      <td>53.50</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.30</td>\n",
       "      <td>-1.19000</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>0.627246</td>\n",
       "      <td>-0.012754</td>\n",
       "      <td>0.234309</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>-0.005623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>53.30</td>\n",
       "      <td>53.40</td>\n",
       "      <td>53.30</td>\n",
       "      <td>53.30</td>\n",
       "      <td>-1.17150</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>0.642491</td>\n",
       "      <td>0.032491</td>\n",
       "      <td>0.229734</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>53.30</td>\n",
       "      <td>53.40</td>\n",
       "      <td>53.20</td>\n",
       "      <td>53.40</td>\n",
       "      <td>-1.15700</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>0.544142</td>\n",
       "      <td>0.154142</td>\n",
       "      <td>0.212436</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>53.30</td>\n",
       "      <td>53.40</td>\n",
       "      <td>52.90</td>\n",
       "      <td>53.00</td>\n",
       "      <td>-1.12900</td>\n",
       "      <td>29.411765</td>\n",
       "      <td>1.004051</td>\n",
       "      <td>-0.135949</td>\n",
       "      <td>0.210778</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34945</th>\n",
       "      <td>112.08</td>\n",
       "      <td>112.52</td>\n",
       "      <td>111.85</td>\n",
       "      <td>112.41</td>\n",
       "      <td>1.04415</td>\n",
       "      <td>49.453552</td>\n",
       "      <td>0.521033</td>\n",
       "      <td>0.893033</td>\n",
       "      <td>0.726014</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.001516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34946</th>\n",
       "      <td>112.43</td>\n",
       "      <td>112.48</td>\n",
       "      <td>111.33</td>\n",
       "      <td>112.09</td>\n",
       "      <td>1.00110</td>\n",
       "      <td>44.783715</td>\n",
       "      <td>0.830014</td>\n",
       "      <td>0.531014</td>\n",
       "      <td>0.713212</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>0.002940</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34947</th>\n",
       "      <td>112.08</td>\n",
       "      <td>112.95</td>\n",
       "      <td>112.02</td>\n",
       "      <td>112.45</td>\n",
       "      <td>0.92620</td>\n",
       "      <td>52.736318</td>\n",
       "      <td>0.450825</td>\n",
       "      <td>0.786825</td>\n",
       "      <td>0.834117</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>-0.003029</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34948</th>\n",
       "      <td>112.40</td>\n",
       "      <td>112.51</td>\n",
       "      <td>112.03</td>\n",
       "      <td>112.15</td>\n",
       "      <td>0.85815</td>\n",
       "      <td>53.132832</td>\n",
       "      <td>0.676968</td>\n",
       "      <td>0.348968</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34949</th>\n",
       "      <td>112.10</td>\n",
       "      <td>112.33</td>\n",
       "      <td>111.49</td>\n",
       "      <td>111.50</td>\n",
       "      <td>0.83045</td>\n",
       "      <td>39.568345</td>\n",
       "      <td>1.364445</td>\n",
       "      <td>-0.246555</td>\n",
       "      <td>0.753804</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>-0.002227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34751 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         open    high     low   close  SMA_delta     RSI_14  BB_delta_cu  \\\n",
       "199     53.50   53.50   53.20   53.20   -1.20850  30.769231     0.705370   \n",
       "200     53.30   53.50   53.20   53.30   -1.19000  35.714286     0.627246   \n",
       "201     53.30   53.40   53.30   53.30   -1.17150  35.714286     0.642491   \n",
       "202     53.30   53.40   53.20   53.40   -1.15700  42.857143     0.544142   \n",
       "203     53.30   53.40   52.90   53.00   -1.12900  29.411765     1.004051   \n",
       "...       ...     ...     ...     ...        ...        ...          ...   \n",
       "34945  112.08  112.52  111.85  112.41    1.04415  49.453552     0.521033   \n",
       "34946  112.43  112.48  111.33  112.09    1.00110  44.783715     0.830014   \n",
       "34947  112.08  112.95  112.02  112.45    0.92620  52.736318     0.450825   \n",
       "34948  112.40  112.51  112.03  112.15    0.85815  53.132832     0.676968   \n",
       "34949  112.10  112.33  111.49  111.50    0.83045  39.568345     1.364445   \n",
       "\n",
       "       BB_delta_cl       ATR    Revers    Moment  target  \n",
       "199      -0.164630  0.224202 -0.000000 -0.003731       2  \n",
       "200      -0.012754  0.234309 -0.001878 -0.005623       0  \n",
       "201       0.032491  0.229734 -0.000000  0.000000       0  \n",
       "202       0.154142  0.212436 -0.000000  0.000000       1  \n",
       "203      -0.135949  0.210778  0.001874  0.001874       0  \n",
       "...            ...       ...       ...       ...     ...  \n",
       "34945     0.893033  0.726014 -0.000089 -0.001516       1  \n",
       "34946     0.531014  0.713212 -0.000178  0.002940       2  \n",
       "34947     0.786825  0.834117  0.000089 -0.003029       0  \n",
       "34948     0.348968  0.788235  0.000445  0.003296       0  \n",
       "34949    -0.246555  0.753804  0.000446 -0.002227       0  \n",
       "\n",
       "[34751 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prepare_for_train(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        # Преобразуем class_weights в тензор и сохраняем\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        targets = targets.long()  # Преобразуем в long для совместимости с CrossEntropyLoss\n",
    "        # Переносим class_weights на устройство, соответствующее targets\n",
    "        weights = self.class_weights.to(targets.device)\n",
    "        # Используем глобальные веса для CrossEntropyLoss\n",
    "        loss = nn.CrossEntropyLoss(weight=weights)(outputs, targets)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1273344651952462, 0.7554027222238209, 0.11726281258093292)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = (df.target.value_counts() / df.shape[0]).values\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class FinancialDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length, target_col):\n",
    "        \"\"\"\n",
    "        data: pandas DataFrame с временными рядами и признаками\n",
    "        sequence_length: длина временной последовательности\n",
    "        target_col: название столбца, который является целевой переменной\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_col = target_col\n",
    "\n",
    "        # Преобразуем данные в тензоры\n",
    "        self.features = torch.tensor(data.drop(columns=[target_col]).values, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(data[target_col].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Количество доступных последовательностей\n",
    "        return len(self.data) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Последовательность признаков\n",
    "        x = self.features[idx:idx + self.sequence_length]\n",
    "        # Целевая переменная для предсказания\n",
    "        y = self.targets[idx + self.sequence_length]\n",
    "        # Добавляем размерность для y\n",
    "        y = torch.tensor([y], dtype=torch.float32)  # Теперь y имеет форму [1]\n",
    "        return x, y\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Прогоняем данные через LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Берем только выход последнего временного шага\n",
    "        lstm_last_out = lstm_out[:, -1, :]\n",
    "        # Полносвязный слой\n",
    "        output = self.fc(lstm_last_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.squeeze())  # Убираем лишнюю размерность в targets\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.squeeze())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesSplit:\n",
    "    def __init__(self, test_size=0.2):\n",
    "        self.test_size = test_size\n",
    "        \n",
    "    def split(self, data):\n",
    "        split_idx = int(len(data)*(1-self.test_size))\n",
    "        return data[:split_idx], data[split_idx:]\n",
    "\n",
    "\n",
    "targets = df.target.values\n",
    "features = df.drop(['target', 'open', 'high', 'low'], axis=1).values \n",
    "\n",
    "# Разделение на тренировочные и валидационные данные\n",
    "x_train, x_val, y_train, y_val = train_test_split(features, targets, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Преобразование в тензоры\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_val = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# Создание Dataset\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "# Создание DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([10, 13]), y shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "target_col = \"target\"  # Название столбца с метками\n",
    "train_dataset = FinancialDataset(data=df, sequence_length=sequence_length, target_col=target_col)\n",
    "\n",
    "x, y = train_dataset[0]\n",
    "print(f\"x shape: {x.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           open      high       low     close\n",
      "open   1.000000  0.999793  0.999785  0.999684\n",
      "high   0.999793  1.000000  0.999637  0.999812\n",
      "low    0.999785  0.999637  1.000000  0.999830\n",
      "close  0.999684  0.999812  0.999830  1.000000\n"
     ]
    }
   ],
   "source": [
    "print(df[['open', 'high', 'low', 'close']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([10, 13])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([16, 10])\n",
      "Targets shape: torch.Size([16])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m# Обучение\u001b[39;00m\n\u001b[0;32m     31\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[1;32m---> 32\u001b[0m trained_model \u001b[39m=\u001b[39m train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
      "Cell \u001b[1;32mIn[62], line 63\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m     61\u001b[0m inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     62\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 63\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     64\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets\u001b[39m.\u001b[39msqueeze())  \u001b[39m# Убираем лишнюю размерность в targets\u001b[39;00m\n\u001b[0;32m     65\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[62], line 43\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(x)\n\u001b[0;32m     42\u001b[0m \u001b[39m# Берем только выход последнего временного шага\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m lstm_last_out \u001b[39m=\u001b[39m lstm_out[:, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, :]\n\u001b[0;32m     44\u001b[0m \u001b[39m# Полносвязный слой\u001b[39;00m\n\u001b[0;32m     45\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(lstm_last_out)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Параметры модели\n",
    "input_size = 10  # Количество признаков\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "class_weights = (df.target.value_counts() / df.shape[0]).values\n",
    "\n",
    "# Инициализация модели\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "criterion = WeightedCrossEntropyLoss(class_weights=class_weights)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-3,\n",
    "    amsgrad=True\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimizer, \n",
    "    base_lr=1e-5,\n",
    "    max_lr=1e-3,\n",
    "    step_size_up=500,\n",
    "    cycle_momentum=False\n",
    ")\n",
    "\n",
    "# Устройство\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Обучение\n",
    "num_epochs = 20\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.2595, Val Loss: 0.2751, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 2/20, Train Loss: 0.2490, Val Loss: 0.2746, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 3/20, Train Loss: 0.2470, Val Loss: 0.2742, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 4/20, Train Loss: 0.2455, Val Loss: 0.2725, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 5/20, Train Loss: 0.2446, Val Loss: 0.2712, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 6/20, Train Loss: 0.2439, Val Loss: 0.2697, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 7/20, Train Loss: 0.2434, Val Loss: 0.2685, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 8/20, Train Loss: 0.2430, Val Loss: 0.2683, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 9/20, Train Loss: 0.2428, Val Loss: 0.2675, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 10/20, Train Loss: 0.2424, Val Loss: 0.2672, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 11/20, Train Loss: 0.2421, Val Loss: 0.2664, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 12/20, Train Loss: 0.2419, Val Loss: 0.2662, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 13/20, Train Loss: 0.2417, Val Loss: 0.2658, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 14/20, Train Loss: 0.2414, Val Loss: 0.2654, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 15/20, Train Loss: 0.2413, Val Loss: 0.2651, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 16/20, Train Loss: 0.2411, Val Loss: 0.2650, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 17/20, Train Loss: 0.2410, Val Loss: 0.2649, Val Accuracy: 0.7487, Val F1 Macro: 0.2854\n",
      "Epoch 18/20, Train Loss: 0.2409, Val Loss: 0.2648, Val Accuracy: 0.7488, Val F1 Macro: 0.2862\n",
      "Epoch 19/20, Train Loss: 0.2408, Val Loss: 0.2648, Val Accuracy: 0.7488, Val F1 Macro: 0.2862\n",
      "Epoch 20/20, Train Loss: 0.2407, Val Loss: 0.2647, Val Accuracy: 0.7488, Val F1 Macro: 0.2862\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score # <---  Добавлен импорт\n",
    "import numpy as np\n",
    "\n",
    "# === Класс для взвешенной функции потерь ===\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        targets = targets.long()  # Преобразуем в long\n",
    "        weights = self.class_weights.to(targets.device)\n",
    "        loss = nn.CrossEntropyLoss(weight=weights)(outputs, targets)\n",
    "        return loss\n",
    "\n",
    "# === Класс для LSTM ===\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_last_out = lstm_out[:, -1, :]  # Последний временной шаг\n",
    "        output = self.fc(lstm_last_out)\n",
    "        return output\n",
    "\n",
    "# === Функция обучения ===\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).long()  # Приведение к long\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_targets = [] # Добавлено для сбора целей валидации\n",
    "        all_val_outputs = [] # Добавлено для сбора предсказаний валидации\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device).long()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                all_val_targets.extend(targets.cpu().numpy()) # Сбор целей\n",
    "                all_val_outputs.extend(torch.argmax(outputs, dim=1).cpu().numpy()) # Сбор предсказаний\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # Расчет метрик на валидационном наборе\n",
    "        val_accuracy = accuracy_score(all_val_targets, all_val_outputs)\n",
    "        val_f1_macro = f1_score(all_val_targets, all_val_outputs, average='macro', zero_division=0) # Важно zero_division=0\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1 Macro: {val_f1_macro:.4f}\") # Вывод метрик\n",
    "\n",
    "    return model\n",
    "\n",
    "# === Подготовка данных ===\n",
    "# Генерация фиктивных данных (замените на ваш DataFrame df)\n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "num_classes = 3\n",
    "\n",
    "np.random.seed(42)\n",
    "features = df.drop(['target', 'open', 'high', 'low'], axis=1).values \n",
    "targets = df.target.values\n",
    "\n",
    "# Разделение на train и val\n",
    "x_train, x_val, y_train, y_val = train_test_split(features, targets, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Преобразование в тензоры\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "x_val = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Создание Dataset и DataLoader\n",
    "train_dataset = TensorDataset(x_train.unsqueeze(1), y_train)  # Добавляем временную размерность\n",
    "val_dataset = TensorDataset(x_val.unsqueeze(1), y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# === Инициализация модели ===\n",
    "input_size = x_train.shape[1]  # Количество признаков\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "class_weights = np.bincount(y_train.numpy()) / len(y_train)\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = WeightedCrossEntropyLoss(class_weights)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-3,\n",
    "    amsgrad=True,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Обучение модели ===\n",
    "num_epochs = 20\n",
    "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train - Loss: 0.7404, Accuracy: 0.7556, F1: 0.2902\n",
      "Val - Loss: 0.8516, Accuracy: 0.7487, F1: 0.2854\n",
      "Epoch 2/100\n",
      "Train - Loss: 0.7207, Accuracy: 0.7572, F1: 0.2889\n",
      "Val - Loss: 0.8796, Accuracy: 0.7006, F1: 0.3359\n",
      "Epoch 3/100\n",
      "Train - Loss: 0.7106, Accuracy: 0.7568, F1: 0.2918\n",
      "Val - Loss: 0.9245, Accuracy: 0.7054, F1: 0.3350\n",
      "Epoch 4/100\n",
      "Train - Loss: 0.7027, Accuracy: 0.7566, F1: 0.2965\n",
      "Val - Loss: 0.9762, Accuracy: 0.7282, F1: 0.3379\n",
      "Epoch 5/100\n",
      "Train - Loss: 0.6983, Accuracy: 0.7572, F1: 0.3018\n",
      "Val - Loss: 1.0367, Accuracy: 0.7261, F1: 0.3358\n",
      "Epoch 6/100\n",
      "Train - Loss: 0.6936, Accuracy: 0.7572, F1: 0.3057\n",
      "Val - Loss: 0.9514, Accuracy: 0.7395, F1: 0.3303\n",
      "Epoch 7/100\n",
      "Train - Loss: 0.6873, Accuracy: 0.7584, F1: 0.3212\n",
      "Val - Loss: 1.1080, Accuracy: 0.7218, F1: 0.3429\n",
      "Epoch 8/100\n",
      "Train - Loss: 0.6860, Accuracy: 0.7581, F1: 0.3266\n",
      "Val - Loss: 1.1035, Accuracy: 0.7300, F1: 0.3405\n",
      "Epoch 9/100\n",
      "Train - Loss: 0.6809, Accuracy: 0.7592, F1: 0.3383\n",
      "Val - Loss: 1.1337, Accuracy: 0.7196, F1: 0.3468\n",
      "Epoch 10/100\n",
      "Train - Loss: 0.6804, Accuracy: 0.7580, F1: 0.3326\n",
      "Val - Loss: 1.0767, Accuracy: 0.7308, F1: 0.3402\n",
      "Epoch 11/100\n",
      "Train - Loss: 0.6775, Accuracy: 0.7582, F1: 0.3377\n",
      "Val - Loss: 1.0834, Accuracy: 0.7353, F1: 0.3601\n",
      "Epoch 12/100\n",
      "Train - Loss: 0.6709, Accuracy: 0.7595, F1: 0.3515\n",
      "Val - Loss: 1.0818, Accuracy: 0.7187, F1: 0.3730\n",
      "Epoch 13/100\n",
      "Train - Loss: 0.6693, Accuracy: 0.7602, F1: 0.3568\n",
      "Val - Loss: 1.0040, Accuracy: 0.7346, F1: 0.3555\n",
      "Epoch 14/100\n",
      "Train - Loss: 0.6702, Accuracy: 0.7600, F1: 0.3569\n",
      "Val - Loss: 1.1113, Accuracy: 0.7162, F1: 0.3816\n",
      "Epoch 15/100\n",
      "Train - Loss: 0.6663, Accuracy: 0.7603, F1: 0.3583\n",
      "Val - Loss: 1.1365, Accuracy: 0.7167, F1: 0.3757\n",
      "Epoch 16/100\n",
      "Train - Loss: 0.6664, Accuracy: 0.7598, F1: 0.3586\n",
      "Val - Loss: 1.0401, Accuracy: 0.7177, F1: 0.3823\n",
      "Epoch 17/100\n",
      "Train - Loss: 0.6627, Accuracy: 0.7620, F1: 0.3674\n",
      "Val - Loss: 1.0390, Accuracy: 0.7244, F1: 0.3795\n",
      "Epoch 18/100\n",
      "Train - Loss: 0.6629, Accuracy: 0.7617, F1: 0.3714\n",
      "Val - Loss: 1.0525, Accuracy: 0.7298, F1: 0.3630\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 157\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m# Обучение модели\u001b[39;00m\n\u001b[0;32m    156\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m model \u001b[39m=\u001b[39m train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
      "Cell \u001b[1;32mIn[79], line 78\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience)\u001b[0m\n\u001b[0;32m     75\u001b[0m inputs, targets \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device), targets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     77\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 78\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     79\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets\u001b[39m.\u001b[39msqueeze())\n\u001b[0;32m     81\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[79], line 45\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m c0 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size)\u001b[39m.\u001b[39mto(x\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     44\u001b[0m \u001b[39m# Forward propagate LSTM\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m lstm_out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h0, c0))\n\u001b[0;32m     47\u001b[0m \u001b[39m# Get last time step output\u001b[39;00m\n\u001b[0;32m     48\u001b[0m last_hidden \u001b[39m=\u001b[39m lstm_out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\n\u001b[0;32m   1124\u001b[0m         \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1125\u001b[0m         hx,\n\u001b[0;32m   1126\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights,\n\u001b[0;32m   1127\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m   1128\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m   1129\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout,\n\u001b[0;32m   1130\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[0;32m   1131\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional,\n\u001b[0;32m   1132\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first,\n\u001b[0;32m   1133\u001b[0m     )\n\u001b[0;32m   1134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        targets = targets.long()\n",
    "        weights = self.class_weights.to(targets.device)\n",
    "        loss = nn.CrossEntropyLoss(weight=weights)(outputs, targets)\n",
    "        return loss\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Get last time step output\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        normalized = self.batch_norm(last_hidden)\n",
    "        \n",
    "        # First dense layer\n",
    "        out = self.fc1(normalized)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience=5):\n",
    "    model = model.to(device)\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.squeeze())\n",
    "            \n",
    "            loss.backward()\n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_metrics = calculate_metrics(np.array(train_targets), np.array(train_predictions))\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.squeeze())\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_metrics = calculate_metrics(np.array(val_targets), np.array(val_predictions))\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_metrics['f1_macro'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1_macro']\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train - Loss: {avg_train_loss:.4f}, Accuracy: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1_macro']:.4f}\")\n",
    "        print(f\"Val - Loss: {avg_val_loss:.4f}, Accuracy: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1_macro']:.4f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', labels=[0, 1, 2], zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, average=None, labels=[0, 1, 2], zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average=None, labels=[0, 1, 2], zero_division=0)\n",
    "    }\n",
    "\n",
    "\n",
    "# Гиперпараметры\n",
    "input_size = 10  # Размерность входных данных\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 3\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Веса классов для несбалансированных данных\n",
    "class_weights = [1.0, 1.0, 1.0]  # Настройте в соответствии с распределением классов\n",
    "\n",
    "# Инициализация модели и оптимизатора\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "criterion = WeightedCrossEntropyLoss(class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Обучение модели\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79112\\AppData\\Local\\Temp\\ipykernel_20068\\1837967402.py:11: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n",
      "[I 2025-01-26 14:35:57,715] A new study created in memory with name: no-name-0a1c9873-2233-4a57-a174-0a93f423182e\n",
      "C:\\Users\\79112\\AppData\\Local\\Temp\\ipykernel_20068\\1310814379.py:43: FutureWarning:\n",
      "\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "\n",
      "[W 2025-01-26 14:35:57,718] Trial 0 failed with parameters: {'hidden_size': 192, 'num_layers': 3, 'learning_rate': 0.0039789128297440515, 'batch_size': 64, 'window_size': 10, 'dropout': 0.5} because of the following error: NameError(\"name 'create_sequences' is not defined\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\79112\\AppData\\Local\\Temp\\ipykernel_20068\\1310814379.py\", line 49, in objective\n",
      "    X, y = create_sequences(scaled_data, window_size) # Используйте trial.suggest_int(\"window_size\", ...)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "NameError: name 'create_sequences' is not defined\n",
      "[W 2025-01-26 14:35:57,719] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m class_weights_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(class_weights, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     92\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, timeout\u001b[39m=\u001b[39;49m\u001b[39m3600\u001b[39;49m) \u001b[39m# Reduced trials for example\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[39mprint\u001b[39m(study\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[82], line 49\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     46\u001b[0m dropout \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39msuggest_float(\u001b[39m\"\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m0.5\u001b[39m, step\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m) \u001b[39m# Dropout rate\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m# Подготовка данных с динамическим window_size\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m X, y \u001b[39m=\u001b[39m create_sequences(scaled_data, window_size) \u001b[39m# Используйте trial.suggest_int(\"window_size\", ...)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m split \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m0.8\u001b[39m \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(X))\n\u001b[0;32m     51\u001b[0m x_train_trial, x_val_trial \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(X[:split], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32), torch\u001b[39m.\u001b[39mtensor(X[split:], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- 1. Усиление взвешивания классов (Weighted Loss - already implemented, showing how to adjust weights) ---\n",
    "# В вашем коде WeightedCrossEntropyLoss уже реализована поддержка весов классов.\n",
    "# Чтобы усилить взвешивание классов меньшинства, нужно изменить значения `class_weights`.\n",
    "\n",
    "# Пример изменения весов классов (увеличьте веса для классов 1 и 2):\n",
    "class_weights = np.array([1.0, 3.0, 3.0]) # Пример: класс 0 - вес 1, классы 1 и 2 - вес 3\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device) # device should be defined already\n",
    "\n",
    "criterion = WeightedCrossEntropyLoss(class_weights=class_weights_tensor) # Используйте обновленные веса при создании criterion\n",
    "\n",
    "\n",
    "# --- 2. Bidirectional LSTM (Обновленный класс LSTMModel) ---\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.0): # Added dropout\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout) # Bidirectional and dropout\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes) # x2 because bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_last_out = lstm_out[:, -1, :]\n",
    "        output = self.fc(lstm_last_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "# --- 3. Optuna с оптимизацией window_size (Обновленная objective функция) ---\n",
    "def objective(trial):\n",
    "    # Гиперпараметры для оптимизации\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 64, 256, step=64)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    window_size = trial.suggest_int(\"window_size\", 10, 60, step=10) # <--- Добавлено window_size в пространство поиска\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5, step=0.1) # Dropout rate\n",
    "\n",
    "    # Подготовка данных с динамическим window_size\n",
    "    X, y = create_sequences(scaled_data, window_size) # Используйте trial.suggest_int(\"window_size\", ...)\n",
    "    split = int(0.8 * len(X))\n",
    "    x_train_trial, x_val_trial = torch.tensor(X[:split], dtype=torch.float32), torch.tensor(X[split:], dtype=torch.float32)\n",
    "    y_train_trial, y_val_trial = torch.tensor(y[:split], dtype=torch.float32), torch.tensor(y[split:], dtype=torch.float32)\n",
    "\n",
    "    # Dataset и DataLoader с динамическим window_size\n",
    "    train_dataset_trial = TensorDataset(x_train_trial, y_train_trial)\n",
    "    val_dataset_trial = TensorDataset(x_val_trial, y_val_trial)\n",
    "    train_loader_trial = DataLoader(train_dataset_trial, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_trial = DataLoader(val_dataset_trial, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    # Модель, Оптимизатор, Критерий\n",
    "    model = LSTMModel(input_size=x_train_trial.shape[2], hidden_size=hidden_size, num_layers=num_layers, num_classes=3, dropout=dropout) # Pass dropout\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-3, amsgrad=True)\n",
    "    criterion = WeightedCrossEntropyLoss(class_weights=class_weights_tensor)\n",
    "\n",
    "    # Обучение модели\n",
    "    num_epochs = 5 # Уменьшено для Optuna\n",
    "    trained_model = train_model(model, train_loader_trial, val_loader_trial, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "    # Валидация и расчет метрики\n",
    "    model.eval()\n",
    "    all_val_preds = []\n",
    "    all_val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader_trial: # Use val_loader_trial\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = trained_model(inputs)\n",
    "            all_val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            all_val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_val_targets, all_val_preds)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# ---- Запуск Optuna ----\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Assuming df, class_weights, train_dataset, val_dataset, x_train, scaled_data are already defined\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10, timeout=3600) # Reduced trials for example\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    print(study.best_trial.params)\n",
    "    print(\"Best Accuracy:\", study.best_value)\n",
    "\n",
    "    best_params = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train - Loss: 1.0848, Accuracy: 0.5025, F1: 0.3472\n",
      "Val   - Loss: 1.0800, Accuracy: 0.5053, F1: 0.3497\n",
      "Epoch 2/100\n",
      "Train - Loss: 1.0764, Accuracy: 0.5263, F1: 0.3655\n",
      "Val   - Loss: 1.0782, Accuracy: 0.5214, F1: 0.3145\n",
      "Epoch 3/100\n",
      "Train - Loss: 1.0722, Accuracy: 0.5464, F1: 0.3698\n",
      "Val   - Loss: 1.0784, Accuracy: 0.5091, F1: 0.3357\n",
      "Epoch 4/100\n",
      "Train - Loss: 1.0704, Accuracy: 0.5369, F1: 0.3731\n",
      "Val   - Loss: 1.0742, Accuracy: 0.5018, F1: 0.3351\n",
      "Epoch 5/100\n",
      "Train - Loss: 1.0697, Accuracy: 0.5379, F1: 0.3760\n",
      "Val   - Loss: 1.0769, Accuracy: 0.4465, F1: 0.3201\n",
      "Epoch 6/100\n",
      "Train - Loss: 1.0670, Accuracy: 0.5405, F1: 0.3798\n",
      "Val   - Loss: 1.0781, Accuracy: 0.5424, F1: 0.3383\n",
      "Epoch 7/100\n",
      "Train - Loss: 1.0663, Accuracy: 0.5489, F1: 0.3858\n",
      "Val   - Loss: 1.0883, Accuracy: 0.4064, F1: 0.2876\n",
      "Epoch 8/100\n",
      "Train - Loss: 1.0656, Accuracy: 0.5456, F1: 0.3837\n",
      "Val   - Loss: 1.0783, Accuracy: 0.4929, F1: 0.3366\n",
      "Epoch 9/100\n",
      "Train - Loss: 1.0647, Accuracy: 0.5463, F1: 0.3798\n",
      "Val   - Loss: 1.0755, Accuracy: 0.4562, F1: 0.3274\n",
      "Epoch 10/100\n",
      "Train - Loss: 1.0640, Accuracy: 0.5540, F1: 0.3892\n",
      "Val   - Loss: 1.0842, Accuracy: 0.4304, F1: 0.3039\n",
      "Epoch 11/100\n",
      "Train - Loss: 1.0616, Accuracy: 0.5478, F1: 0.3891\n",
      "Val   - Loss: 1.0942, Accuracy: 0.3991, F1: 0.2860\n",
      "Ранняя остановка активирована\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79112\\AppData\\Local\\Temp\\ipykernel_20068\\3906777930.py:124: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        targets = targets.long()\n",
    "        weights = self.class_weights.to(targets.device)\n",
    "        loss = nn.CrossEntropyLoss(weight=weights)(outputs, targets)\n",
    "        return loss\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Инициализация скрытого и клеточного состояний\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Прямой проход через LSTM\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Получение последнего временного шага\n",
    "        last_hidden = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Пропуск через Dropout\n",
    "        out = self.dropout(last_hidden)\n",
    "        \n",
    "        # Выходной слой\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience=10):\n",
    "    model = model.to(device)\n",
    "    best_val_f1 = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Фаза обучения\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.squeeze())\n",
    "            \n",
    "            loss.backward()\n",
    "            # Ограничение градиентов для предотвращения взрыва градиентов\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_metrics = calculate_metrics(np.array(train_targets), np.array(train_predictions))\n",
    "\n",
    "        # Фаза валидации\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.squeeze())\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_metrics = calculate_metrics(np.array(val_targets), np.array(val_predictions))\n",
    "\n",
    "        # Раннее прекращение обучения\n",
    "        if val_metrics['f1_macro'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1_macro']\n",
    "            patience_counter = 0\n",
    "            # Сохранение лучшей модели\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train - Loss: {avg_train_loss:.4f}, Accuracy: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1_macro']:.4f}\")\n",
    "        print(f\"Val   - Loss: {avg_val_loss:.4f}, Accuracy: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1_macro']:.4f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Ранняя остановка активирована\")\n",
    "            break\n",
    "\n",
    "    # Загрузка лучшей модели\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    return model\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro', labels=[0, 1, 2], zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, average=None, labels=[0, 1, 2], zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average=None, labels=[0, 1, 2], zero_division=0)\n",
    "    }\n",
    "\n",
    "def prepare_data(file_paths):\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "    features = df.drop(['target', 'open', 'high', 'low'], axis=1).values \n",
    "    targets = df.target.values\n",
    "    \n",
    "    # Нормализация\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    \n",
    "    all_data = [features, targets]\n",
    "    \n",
    "    # Объединение всех данных\n",
    "    X = all_data[0]\n",
    "    y = all_data[1]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def create_data_loaders(X, y, sequence_length=10, batch_size=32, split_ratio=0.8):\n",
    "    # Создание последовательностей для LSTM\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(X) - sequence_length):\n",
    "        sequences.append(X[i:i+sequence_length])\n",
    "        targets.append(y[i+sequence_length])\n",
    "    \n",
    "    sequences = np.array(sequences)\n",
    "    targets = np.array(targets)\n",
    "    \n",
    "    # Разделение на обучающую и валидационную выборки\n",
    "    split = int(len(sequences) * split_ratio)\n",
    "    X_train, X_val = sequences[:split], sequences[split:]\n",
    "    y_train, y_val = targets[:split], targets[split:]\n",
    "    \n",
    "    # Создание TensorDataset\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    \n",
    "    # Создание DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Основной блок кода\n",
    "if __name__ == \"__main__\":\n",
    "    # Пути к файлам с данными\n",
    "    file_paths = sv_files = [f for f in os.listdir(\n",
    "        \"./data/\") if os.path.isfile(os.path.join(\"./data/\", f))]\n",
    "    \n",
    "    # Подготовка данных\n",
    "    X, y = prepare_data(file_paths)\n",
    "    \n",
    "    # Создание DataLoader\n",
    "    train_loader, val_loader = create_data_loaders(X, y, sequence_length=10, batch_size=32, split_ratio=0.8)\n",
    "    \n",
    "    # Гиперпараметры\n",
    "    input_size = X.shape[1]  # Количество признаков\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    num_classes = 3\n",
    "    learning_rate = 0.0005\n",
    "    num_epochs = 100\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Рассчёт весов классов для обработки дисбаланса\n",
    "    class_counts = np.bincount(y)\n",
    "    class_weights = 1. / (class_counts + 1e-6)  # Добавление малой величины для избежания деления на ноль\n",
    "    class_weights = class_weights / class_weights.sum() * len(class_weights)  # Нормализация\n",
    "    \n",
    "    # Инициализация модели и оптимизатора\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout=0.3)\n",
    "    criterion = WeightedCrossEntropyLoss(class_weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Обучение модели\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, patience=10)\n",
    "    \n",
    "    # Сохранение финальной модели\n",
    "    torch.save(model.state_dict(), 'final_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "# Предположим, что входные признаки в колонках 'feature1', 'feature2', ..., а цель в колонках 'target_0', 'target_1', 'target_2'\n",
    "features = df.iloc[:, 3:].values\n",
    "targets = df.iloc[:, :3].values  # One-hot encoded target\n",
    "\n",
    "# Разделение на тренировочные и валидационные данные\n",
    "x_train, x_val, y_train, y_val = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Преобразование в тензоры\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "x_val = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (34751, 11)\n",
      "Shape of y: (34751, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of x:\", features.shape)  # Ожидается (N, 11)\n",
    "print(\"Shape of y:\", targets.shape)  # Ожидается (N, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание Dataset\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "# Создание DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[27800, 1, 1]' is invalid for input of size 305800",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[0;32m     19\u001b[0m seq_length \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# Если данные не являются последовательностями, можно использовать seq_length = 1\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m x_train \u001b[39m=\u001b[39m x_train\u001b[39m.\u001b[39;49mview(x_train\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], seq_length, x_train\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     21\u001b[0m x_val \u001b[39m=\u001b[39m x_val\u001b[39m.\u001b[39mview(x_val\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], seq_length, x_val\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m     23\u001b[0m \u001b[39m# Параметры модели\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[27800, 1, 1]' is invalid for input of size 305800"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class PricePredictionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(PricePredictionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # Выходное число классов\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # Последний выход LSTM\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "seq_length = 1  # Если данные не являются последовательностями, можно использовать seq_length = 1\n",
    "x_train = x_train.view(x_train.shape[0], seq_length, x_train.shape[1])\n",
    "x_val = x_val.view(x_val.shape[0], seq_length, x_val.shape[1])\n",
    "\n",
    "# Параметры модели\n",
    "input_dim = x_train.shape[2]  # Количество признаков (feature_size)\n",
    "hidden_dim = 64\n",
    "output_dim = 3  # Количество классов\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Создаем модель\n",
    "model = PricePredictionLSTM(input_size=input_dim, hidden_size=hidden_dim, num_classes=output_dim)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Пример вычисления ошибки\n",
    "y_train_classes = torch.argmax(y_train, dim=1)  # Преобразование one-hot в индексы\n",
    "outputs = model(x_train)\n",
    "loss = loss_fn(outputs, y_train_classes)\n",
    "\n",
    "# Оптимизатор\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Устройство\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Обучение модели\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Добавляем измерение для sequence_length\n",
    "        x_batch = x_batch.unsqueeze(1)\n",
    "\n",
    "        # Преобразование меток\n",
    "        y_batch_classes = torch.argmax(y_batch, dim=1)\n",
    "\n",
    "        # Прямой проход\n",
    "        outputs = model(x_batch)\n",
    "        loss = loss_fn(outputs, y_batch_classes)\n",
    "\n",
    "        # Обратное распространение\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_val_batch, y_val_batch in val_loader:\n",
    "            x_val_batch, y_val_batch = x_val_batch.to(device), y_val_batch.to(device)\n",
    "\n",
    "            # Добавляем измерение для sequence_length\n",
    "            x_val_batch = x_val_batch.unsqueeze(1)\n",
    "\n",
    "            y_val_classes = torch.argmax(y_val_batch, dim=1)\n",
    "            outputs = model(x_val_batch)\n",
    "            val_loss += loss_fn(outputs, y_val_classes).item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss / len(val_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27800, 1, 11])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Определение целевой функции\n",
    "def objective(trial):\n",
    "    # Гиперпараметры, которые будет выбирать Optuna\n",
    "    input_size = 11  # Количество фичей (OHLC + индикаторы)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 256, step=32)  # Скрытые нейроны\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)  # Количество слоев\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5, step=0.1)  # Dropout\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)  # LR\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 128, step=16)  # Размер батча\n",
    "\n",
    "    # Создаем модель\n",
    "    model = PricePredictionLSTM(input_size, hidden_size, num_layers, output_size=3, dropout=dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    # Оптимизатор\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Загрузка данных\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Обучение модели\n",
    "    for epoch in range(10):  # Число эпох (можно увеличить)\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "            preds = model(X_val).argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(y_val.cpu().numpy())\n",
    "\n",
    "    # Возвращаем метрику для минимизации (например, -accuracy для максимизации)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")  # Максимизация accuracy\n",
    "study.optimize(objective, n_trials=50)  # Количество попыток\n",
    "\n",
    "# Лучшая комбинация гиперпараметров\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_trial.params\n",
    "\n",
    "# Создаем модель с лучшими параметрами\n",
    "final_model = PricePredictionLSTM(\n",
    "    input_size=11,\n",
    "    hidden_size=best_params[\"hidden_size\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    output_size=3,\n",
    "    dropout=best_params[\"dropout\"],\n",
    ")\n",
    "final_model.to(device)\n",
    "\n",
    "# Обучаем модель заново на всех данных с лучшими параметрами\n",
    "train_loader = DataLoader(full_train_dataset, batch_size=best_params[\"batch_size\"], shuffle=True)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "# ... (Обучение модели как ранее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.92 MiB for an array with shape (11, 34751) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 54\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(X), np\u001b[39m.\u001b[39marray(y)\n\u001b[0;32m     53\u001b[0m window_size \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m\n\u001b[1;32m---> 54\u001b[0m X, y \u001b[39m=\u001b[39m create_sequences(df, window_size)\n\u001b[0;32m     56\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(\n\u001b[0;32m     57\u001b[0m     X, \n\u001b[0;32m     58\u001b[0m     y,\n\u001b[0;32m     59\u001b[0m     test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m,\n\u001b[0;32m     60\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m  \u001b[39m# Для временных рядов\u001b[39;00m\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     62\u001b[0m \u001b[39m# 3. Class Weight Calculation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 48\u001b[0m, in \u001b[0;36mcreate_sequences\u001b[1;34m(data, window_size)\u001b[0m\n\u001b[0;32m     46\u001b[0m X, y \u001b[39m=\u001b[39m [], []\n\u001b[0;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data)\u001b[39m-\u001b[39mwindow_size):\n\u001b[1;32m---> 48\u001b[0m     X\u001b[39m.\u001b[39mappend(data\u001b[39m.\u001b[39;49miloc[i:i\u001b[39m+\u001b[39;49mwindow_size, \u001b[39m3\u001b[39;49m:]\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m     49\u001b[0m     y\u001b[39m.\u001b[39mappend(data\u001b[39m.\u001b[39miloc[i\u001b[39m+\u001b[39mwindow_size, :\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39mvalues)\n\u001b[0;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(X), np\u001b[39m.\u001b[39marray(y)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\indexing.py:1147\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1146\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[1;32m-> 1147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[0;32m   1148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\indexing.py:1656\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1653\u001b[0m \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1654\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_lowerdim(tup)\n\u001b[1;32m-> 1656\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\indexing.py:994\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_null_slice(key):\n\u001b[0;32m    992\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 994\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(retval, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\u001b[39m.\u001b[39;49m_getitem_axis(key, axis\u001b[39m=\u001b[39;49mi)\n\u001b[0;32m    995\u001b[0m \u001b[39m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \u001b[39m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[0;32m    997\u001b[0m \u001b[39massert\u001b[39;00m retval\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\indexing.py:1691\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1685\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\n\u001b[0;32m   1686\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDataFrame indexer is not allowed for .iloc\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1687\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConsider using .loc for automatic alignment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1688\u001b[0m     )\n\u001b[0;32m   1690\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mslice\u001b[39m):\n\u001b[1;32m-> 1691\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_slice_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   1693\u001b[0m \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   1694\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\indexing.py:1727\u001b[0m, in \u001b[0;36m_iLocIndexer._get_slice_axis\u001b[1;34m(self, slice_obj, axis)\u001b[0m\n\u001b[0;32m   1725\u001b[0m labels \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1726\u001b[0m labels\u001b[39m.\u001b[39m_validate_positional_slice(slice_obj)\n\u001b[1;32m-> 1727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_slice(slice_obj, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\generic.py:4304\u001b[0m, in \u001b[0;36mNDFrame._slice\u001b[1;34m(self, slobj, axis)\u001b[0m\n\u001b[0;32m   4302\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(slobj, \u001b[39mslice\u001b[39m), \u001b[39mtype\u001b[39m(slobj)\n\u001b[0;32m   4303\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_block_manager_axis(axis)\n\u001b[1;32m-> 4304\u001b[0m new_mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mget_slice(slobj, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[0;32m   4305\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_from_mgr(new_mgr, axes\u001b[39m=\u001b[39mnew_mgr\u001b[39m.\u001b[39maxes)\n\u001b[0;32m   4306\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32minternals.pyx:927\u001b[0m, in \u001b[0;36mpandas._libs.internals.BlockManager.get_slice\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:826\u001b[0m, in \u001b[0;36mBaseBlockManager._slice_take_blocks_ax0\u001b[1;34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[0m\n\u001b[0;32m    824\u001b[0m                     blocks\u001b[39m.\u001b[39mappend(nb)\n\u001b[0;32m    825\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 826\u001b[0m                 nb \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39;49mtake_nd(taker, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, new_mgr_locs\u001b[39m=\u001b[39;49mmgr_locs)\n\u001b[0;32m    827\u001b[0m                 blocks\u001b[39m.\u001b[39mappend(nb)\n\u001b[0;32m    829\u001b[0m \u001b[39mreturn\u001b[39;00m blocks\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:1061\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     allow_fill \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[39m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1061\u001b[0m new_values \u001b[39m=\u001b[39m algos\u001b[39m.\u001b[39;49mtake_nd(\n\u001b[0;32m   1062\u001b[0m     values, indexer, axis\u001b[39m=\u001b[39;49maxis, allow_fill\u001b[39m=\u001b[39;49mallow_fill, fill_value\u001b[39m=\u001b[39;49mfill_value\n\u001b[0;32m   1063\u001b[0m )\n\u001b[0;32m   1065\u001b[0m \u001b[39m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[39m#  these assertions\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1068\u001b[0m     \u001b[39m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m     \u001b[39m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:118\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mtake(indexer, fill_value\u001b[39m=\u001b[39mfill_value, allow_fill\u001b[39m=\u001b[39mallow_fill)\n\u001b[0;32m    117\u001b[0m arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(arr)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m _take_nd_ndarray(arr, indexer, axis, fill_value, allow_fill)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\pandas\\core\\array_algos\\take.py:158\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    156\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(out_shape, dtype\u001b[39m=\u001b[39mdtype, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     out \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(out_shape, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    160\u001b[0m func \u001b[39m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    161\u001b[0m     arr\u001b[39m.\u001b[39mndim, arr\u001b[39m.\u001b[39mdtype, out\u001b[39m.\u001b[39mdtype, axis\u001b[39m=\u001b[39maxis, mask_info\u001b[39m=\u001b[39mmask_info\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    163\u001b[0m func(arr, indexer, out, fill_value)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.92 MiB for an array with shape (11, 34751) and data type float64"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. Custom Loss and Metrics\n",
    "class BalancedCategoricalLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, class_weights, name=\"balanced_categorical_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.class_weights = tf.constant(class_weights, dtype=tf.float32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        weights = tf.reduce_sum(y_true * self.class_weights, axis=1)\n",
    "        ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "        return tf.reduce_mean(ce * weights)\n",
    "\n",
    "class F1Metrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val.argmax(axis=1)\n",
    "        self.classes = np.unique(np.concatenate([self.y_val, [0, 1, 2]]))  # Гарантируем все классы\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(self.X_val).argmax(axis=1)\n",
    "        \n",
    "        # Добавляем защиту от отсутствующих классов\n",
    "        present_classes = np.unique(np.concatenate([y_pred, self.y_val]))\n",
    "        f1 = f1_score(self.y_val, y_pred, average=None, labels=[0, 1, 2], zero_division=0)\n",
    "        \n",
    "        logs['val_f1_0'] = f1[0]\n",
    "        logs['val_f1_1'] = f1[1] if len(f1) > 1 else 0\n",
    "        logs['val_f1_2'] = f1[2] if len(f1) > 2 else 0\n",
    "        \n",
    "        print(f\"\\nVal F1: [0: {f1[0]:.3f}, -1: {f1[1]:.3f} (1), +1: {f1[2]:.3f} (2)] | Classes present: {present_classes}\")\n",
    "\n",
    "# 2. Data Preparation\n",
    "def create_sequences(data, window_size=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data)-window_size):\n",
    "        X.append(data.iloc[i:i+window_size, 3:].values)\n",
    "        y.append(data.iloc[i+window_size, :3].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "window_size = 15\n",
    "X, y = create_sequences(df, window_size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    shuffle=False  # Для временных рядов\n",
    ")\n",
    "# 3. Class Weight Calculation\n",
    "class_counts = np.array([26251, 4425, 4075])\n",
    "class_weights = (1 / class_counts) * (class_counts.sum() / 3)\n",
    "class_weights /= class_weights.sum()  # Нормализация\n",
    "\n",
    "# 4. Model Architecture\n",
    "model = Sequential([\n",
    "    LSTM(32, input_shape=(window_size, X.shape[2]), \n",
    "         kernel_regularizer=l2(0.005), recurrent_regularizer=l2(0.005)),\n",
    "    Dropout(0.4),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# 5. Model Compilation\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=BalancedCategoricalLoss(class_weights),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 6. Training Setup\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_f1_1', patience=10, mode='max', restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n",
    "    F1Metrics(X_val=X_test, y_val=y_test)\n",
    "]\n",
    "\n",
    "# 7. Training Execution\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=callbacks,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 8. Post-Training Analysis\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss Evolution')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['val_f1_0'], label='F1 Class 0')\n",
    "    plt.plot(history.history['val_f1_1'], label='F1 Class -1')\n",
    "    plt.plot(history.history['val_f1_2'], label='F1 Class +1')\n",
    "    plt.title('F1 Scores')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "# 9. Prediction and Risk Management\n",
    "def calculate_levels(entry_price, atr, min_range=0.0035, max_range=0.02):\n",
    "    range_size = np.clip(atr, min_range, max_range)\n",
    "    return {\n",
    "        'stop_loss': entry_price * (1 - range_size),\n",
    "        'take_profit': entry_price * (1 + range_size)\n",
    "    }\n",
    "\n",
    "# 10. Model Evaluation\n",
    "def full_evaluation(model, X, y):\n",
    "    y_pred = model.predict(X).argmax(axis=1)\n",
    "    y_true = y.argmax(axis=1)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    \n",
    "    # Визуализация распределения предсказаний\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(y_pred, bins=[-0.5, 0.5, 1.5, 2.5], alpha=0.7)\n",
    "    plt.xticks([0, 1, 2], ['0', '-1', '+1'])\n",
    "    plt.title(\"Prediction Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "full_evaluation(model, X_test, y_test)\n",
    "\n",
    "# 11. Adaptive Trading System\n",
    "def trading_simulation(model, data, initial_balance=10000):\n",
    "    balance = initial_balance\n",
    "    position = None\n",
    "    trade_log = []\n",
    "    \n",
    "    for i in range(len(data)-window_size):\n",
    "        sequence = data.iloc[i:i+window_size, 3:].values.reshape(1, window_size, -1)\n",
    "        prediction = model.predict(sequence).argmax()\n",
    "        current_price = data.iloc[i+window_size]['Close']\n",
    "        \n",
    "        # Логика управления позицией\n",
    "        if position is None:\n",
    "            if prediction == 1:  # Long\n",
    "                position = {\n",
    "                    'type': 'long',\n",
    "                    'entry': current_price,\n",
    "                    'sl': current_price * 0.997,\n",
    "                    'tp': current_price * 1.003\n",
    "                }\n",
    "            elif prediction == 2:  # Short\n",
    "                position = {\n",
    "                    'type': 'short',\n",
    "                    'entry': current_price,\n",
    "                    'sl': current_price * 1.003,\n",
    "                    'tp': current_price * 0.997\n",
    "                }\n",
    "        else:\n",
    "            # Проверка условий закрытия\n",
    "            if (position['type'] == 'long' and \n",
    "               (current_price <= position['sl'] or current_price >= position['tp'])):\n",
    "                pnl = (current_price - position['entry']) / position['entry']\n",
    "                balance *= (1 + pnl)\n",
    "                trade_log.append(pnl)\n",
    "                position = None\n",
    "                \n",
    "            elif (position['type'] == 'short' and \n",
    "                 (current_price >= position['sl'] or current_price <= position['tp'])):\n",
    "                pnl = (position['entry'] - current_price) / position['entry']\n",
    "                balance *= (1 + pnl)\n",
    "                trade_log.append(pnl)\n",
    "                position = None\n",
    "                \n",
    "    return balance, trade_log\n",
    "\n",
    "# Запуск симуляции\n",
    "final_balance, trades = trading_simulation(model, df)\n",
    "print(f\"Final balance: ${final_balance:.2f}\")\n",
    "print(f\"Total trades: {len(trades)}\")\n",
    "print(f\"Win rate: {sum(1 for x in trades if x > 0)/len(trades)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_analysis(model, X_test, y_test):\n",
    "    baseline_score = model.evaluate(X_test, y_test, verbose=0)[1]\n",
    "    importance = np.zeros((X.shape[2],))\n",
    "    \n",
    "    for i in range(X.shape[2]):\n",
    "        X_temp = X_test.copy()\n",
    "        np.random.shuffle(X_temp[:, :, i])\n",
    "        shuffled_score = model.evaluate(X_temp, y_test, verbose=0)[1]\n",
    "        importance[i] = baseline_score - shuffled_score\n",
    "    \n",
    "    return importance\n",
    "\n",
    "feature_importance = feature_importance_analysis(model, X_test, y_test)\n",
    "selected_features = np.where(feature_importance > 0.005)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = df.columns[3:][selected_features]  # Получаем названия фичей\n",
    "filtered_df = df.iloc[:, list(range(3)) + selected_features.tolist()]  # Новый датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '—' (U+2014) (3141982048.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[397], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    Мне необходимо разработать алгоритм-индикатор, который прогнозирует краткосрочные ценовые изменения выбранного финансового инструмента. Основная цель — предсказать направление (рост, падение или стабильность) цены на 15-минутный временной интервал с максимально возможной точностью. Модель должна генерировать рекомендации по уровням stop-loss и take-profit для каждой сделки.\u001b[0m\n\u001b[1;37m                                                                                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '—' (U+2014)\n"
     ]
    }
   ],
   "source": [
    "Сейчас я тебе предоставлю контекст для решения задачи, а в дальнейшем ты будешь должен ответить на мои вопросы и в итоге совместно со мной написать решение задачи на языке Python.\n",
    "\n",
    "Мне необходимо разработать алгоритм-индикатор, который прогнозирует краткосрочные ценовые изменения выбранного финансового инструмента. Основная цель — предсказать направление (рост, падение или стабильность) цены на 15-минутный временной интервал с максимально возможной точностью. Модель должна генерировать рекомендации по уровням stop-loss и take-profit для каждой сделки.\n",
    "\n",
    "Входные данные: В качестве данных используется выгрузка с биржи о движении цен в формате CSV. Данные хранятся в 10 файлах, которые содержат Open High, Low, Close и соответствующий timestamp для 10 различных активов. В каждом файле данные о конкретном активе. Суммарный объем данных составляет около 16 Мб. Данные предоставляются в таймфрейме 15-минут (интервал между данными составляет 15 минут).\n",
    "\n",
    "Требования к решению (в порядке убывания приоритета):\n",
    "\n",
    "Предсказать направление цены (+1: рост, -1: падение, 0: стабильность на следующий временной интервал.\n",
    "\n",
    "Рассчитать оптимальные уровни stop-loss и take-profit для минимизации риска и максимизации прибыли.\n",
    "\n",
    "Сделки должны идти непрерывно: одна сделка закрылась - следующая открылась.\n",
    "\n",
    "Решение будет оцениваться в соответствии со следующими метриками (в порядке убывания приоритета):\n",
    "\n",
    "Win-rate – процент прибыльных сделок.\n",
    "\n",
    "PnL (Profit and Loss) – итоговая прибыль/убыток в процентах за определенный период.\n",
    "\n",
    "Количество сделок – количество всех сделок, совершенных в процессе торговли.\n",
    "\n",
    "Максимальная просадка депозита – максимальное снижение капитала за весь период торговли.\n",
    "\n",
    "Средний процент прибыли на сделку – средний процент, полученный за каждую прибыльную сделку.\n",
    "\n",
    "Средняя ширина от точки входа до выхода – средний процент изменения цены от входа в сделку до ее завершения.\n",
    "\n",
    "Дополнительные условия:\n",
    "\n",
    "Ограничений на выбор алгоритмов нет, но крайне приветствуются интерпретируемые модели и модели машинного обучения.\n",
    "\n",
    "Stop-loss и take-profit должны быть рассчитаны на основе исторических данных, волатильности, прогнозов модели и других параметров. А ширина от точки входа до stop-loss и take-profit ордеров должна быть фиксирована (risk/reward 1:1). Причем, минимальный диапазон 0.35%, максимальный 2%.\n",
    "\n",
    "Таким образом, цель задания - разработать алгоритм, который анализирует данные о движении цены криптовалют и автоматически рассчитывает возможные уровни для входа в сделку, стоп-лосса и тейк-профита.\n",
    "\n",
    "Необязательные, но важные предпочтения:\n",
    "\n",
    "Минимальная задержка в обработке данных (реализация модели должна быть пригодна для реального времени).\n",
    "\n",
    "Реализация адаптивного расчета stop-loss и take-profit с учетом изменения волатильности в реальном времени\n",
    "\n",
    "Нормализация данных (избавление от шумов и аномальных движений).\n",
    "\n",
    "В качестве дополнительных фичей, помимо High, Open, Close и Low,  я собираюсь использовать следующие индикаторы: SMA_delta (Дельта скользящих), RSI_14 (Индекс относительной силы), BB_delta_cu, BB_delta_cl, ATR, Mean-reversion, Momentum. Я выбрал стратегию прогнозирования High/Low, а не Close-to-Close. Я хочу обучить LSTM для этих целей. \n",
    "\n",
    "Твои задачи:\n",
    "1. Придумать и разработаь функцию потерь, которая будет максимизировать Win Rate, то есть % верных предсказаний +1, -1 и 0 от общего количества предсказаний.\n",
    "2. Напиши код для обучения LSTM\n",
    "3. Напиши код, который будет оптимизировать гиперпараметры модели с помощью Optuna, Hyperopt или подобного инструмента.\n",
    "4. Напиши код, который будет отбирать фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-26 11:10:47,159] A new study created in memory with name: no-name-3f4a4d24-738e-46e9-83d6-9741bde306df\n",
      "[W 2025-01-26 11:12:04,728] Trial 0 failed with parameters: {'window_size': 85, 'lstm_units1': 256, 'lstm_units2': 32, 'dropout_rate': 0.24109241705728152, 'learning_rate': 3.425496174487925e-05, 'gamma': 1.1859461869076606, 'batch_size': 128, 'loss_weights_dir': 0.7791428516518702} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\79112\\AppData\\Local\\Temp\\ipykernel_12104\\2727031208.py\", line 129, in objective\n",
      "    history = model.fit(\n",
      "              ^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 371, in fit\n",
      "    logs = self.train_function(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 219, in function\n",
      "    opt_outputs = multi_step_on_iterator(iterator)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 833, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\", line 878, in _call\n",
      "    results = tracing_compilation.call_function(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py\", line 139, in call_function\n",
      "    return function._call_flat(  # pylint: disable=protected-access\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py\", line 1322, in _call_flat\n",
      "    return self._inference_function.call_preflattened(args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 216, in call_preflattened\n",
      "    flat_outputs = self.call_flat(*args)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py\", line 251, in call_flat\n",
      "    outputs = self._bound_context.call_function(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 1683, in call_function\n",
      "    outputs = execute.execute(\n",
      "              ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py\", line 53, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-01-26 11:12:04,954] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 161\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39m# Оптимизация\u001b[39;00m\n\u001b[0;32m    160\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m, sampler\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler())\n\u001b[1;32m--> 161\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, timeout\u001b[39m=\u001b[39;49m\u001b[39m3600\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m    163\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mЛучшие параметры: \u001b[39m\u001b[39m{\u001b[39;00mstudy\u001b[39m.\u001b[39mbest_params\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    164\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mЛучший результат: \u001b[39m\u001b[39m{\u001b[39;00mstudy\u001b[39m.\u001b[39mbest_value\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[11], line 129\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m    107\u001b[0m model \u001b[39m=\u001b[39m create_enhanced_model(\n\u001b[0;32m    108\u001b[0m     input_shape\u001b[39m=\u001b[39m(hp[\u001b[39m'\u001b[39m\u001b[39mwindow_size\u001b[39m\u001b[39m'\u001b[39m], data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]),\n\u001b[0;32m    109\u001b[0m     lstm_units1\u001b[39m=\u001b[39mhp[\u001b[39m'\u001b[39m\u001b[39mlstm_units1\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    110\u001b[0m     lstm_units2\u001b[39m=\u001b[39mhp[\u001b[39m'\u001b[39m\u001b[39mlstm_units2\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m    111\u001b[0m     dropout_rate\u001b[39m=\u001b[39mhp[\u001b[39m'\u001b[39m\u001b[39mdropout_rate\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    112\u001b[0m )\n\u001b[0;32m    114\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m    115\u001b[0m     optimizer\u001b[39m=\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mhp[\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[0;32m    116\u001b[0m     loss\u001b[39m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    126\u001b[0m     metrics\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mdirection\u001b[39m\u001b[39m'\u001b[39m: [DirectionAccuracy()]}\n\u001b[0;32m    127\u001b[0m )\n\u001b[1;32m--> 129\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    130\u001b[0m     X_train, y_train,\n\u001b[0;32m    131\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_val, y_val),\n\u001b[0;32m    132\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m    133\u001b[0m     batch_size\u001b[39m=\u001b[39;49mhp[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    134\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[0;32m    135\u001b[0m         TFKerasPruningCallback(trial, \u001b[39m'\u001b[39;49m\u001b[39mval_direction_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m    136\u001b[0m         EarlyStopping(monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_direction_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m, patience\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    137\u001b[0m     ],\n\u001b[0;32m    138\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[0;32m    139\u001b[0m )\n\u001b[0;32m    141\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_direction_accuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39mfor\u001b[39;00m step, iterator \u001b[39min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mIterator, tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[39m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    220\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_outputs\u001b[39m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1684\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1685\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1686\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1687\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1688\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1689\u001b[0m   )\n\u001b[0;32m   1690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, concatenate, BatchNormalization, LayerNormalization, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Кастомная метрика для точности направления\n",
    "class DirectionAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='direction_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.correct = self.add_weight(name='correct', initializer='zeros')\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_class = tf.argmax(y_pred, axis=1)\n",
    "        y_true_class = tf.argmax(y_true, axis=1)\n",
    "        matches = tf.cast(tf.equal(y_pred_class, y_true_class), tf.float32)\n",
    "        self.correct.assign_add(tf.reduce_sum(matches))\n",
    "        self.total.assign_add(tf.cast(tf.size(matches), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct / self.total\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.correct.assign(0.0)\n",
    "        self.total.assign(0.0)\n",
    "\n",
    "# 2. Улучшенная функция потерь\n",
    "def asymmetric_focal_loss(y_true, y_pred, alpha=[1.0, 1.0, 1.0], gamma=2.0):\n",
    "    ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    probs = tf.reduce_max(y_true * y_pred, axis=1)\n",
    "    alpha_weights = tf.reduce_sum(y_true * alpha, axis=1)\n",
    "    focal_loss = alpha_weights * (1 - probs)**gamma * ce_loss\n",
    "    return tf.reduce_mean(focal_loss)\n",
    "\n",
    "# 3. Генератор временных окон с аугментацией\n",
    "def create_sequences(data, window_size, step=1):\n",
    "    sequences = []\n",
    "    targets_dir = []\n",
    "    targets_high = []\n",
    "    targets_low = []\n",
    "\n",
    "    for i in range(window_size, len(data)-1, step):\n",
    "        sequences.append(data[i-window_size:i])\n",
    "        targets_dir.append(data[i, 0:3])\n",
    "        targets_high.append(data[i+1, 3])  # Прогноз на следующий шаг\n",
    "        targets_low.append(data[i+1, 4])\n",
    "\n",
    "    return (np.array(sequences),\n",
    "            [np.array(targets_dir),\n",
    "             np.array(targets_high).reshape(-1, 1),\n",
    "             np.array(targets_low).reshape(-1, 1)])\n",
    "\n",
    "# 4. Улучшенная архитектура модели\n",
    "def create_enhanced_model(input_shape, lstm_units1, lstm_units2, dropout_rate):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Временная ветвь с Bidirectional LSTM\n",
    "    x = Bidirectional(LSTM(lstm_units1, return_sequences=True, dropout=dropout_rate))(inputs)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = Bidirectional(LSTM(lstm_units2, dropout=dropout_rate))(x)\n",
    "\n",
    "    # Технические индикаторы\n",
    "    tech = inputs[:, -1, -3:]\n",
    "    tech = Dense(32, activation='swish')(tech)\n",
    "\n",
    "    # Объединение\n",
    "    merged = concatenate([x, tech])\n",
    "    merged = Dense(128, activation='swish')(merged)\n",
    "    merged = Dropout(dropout_rate)(merged)\n",
    "\n",
    "    # Выходы\n",
    "    direction = Dense(3, activation='softmax', name='direction')(merged)    \n",
    "    high = Dense(1, activation='linear', name='high')(merged)\n",
    "    low = Dense(1, activation='linear', name='low')(merged)\n",
    "\n",
    "    return Model(inputs, [direction, high, low])\n",
    "\n",
    "# 5. Оптимизация гиперпараметров\n",
    "def objective(trial):\n",
    "    # Гиперпараметры\n",
    "    hp = {\n",
    "        'window_size': trial.suggest_int('window_size', 30, 90),\n",
    "        'lstm_units1': trial.suggest_categorical('lstm_units1', [64, 128, 256]),\n",
    "        'lstm_units2': trial.suggest_categorical('lstm_units2', [32, 64, 128]),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.2, 0.5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 1.0, 3.0),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [64, 128]),\n",
    "        'loss_weights_dir': trial.suggest_float('loss_weights_dir', 0.4, 0.8)\n",
    "    }\n",
    "\n",
    "    # Подготовка данных\n",
    "    X, y = create_sequences(scaled_data, hp['window_size'])\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, X_val = X[:split], X[split:]\n",
    "    y_train, y_val = [y[0][:split], y[1][:split], y[2][:split]], \\\n",
    "                     [y[0][split:], y[1][split:], y[2][split:]]\n",
    "    \n",
    "    # Модель\n",
    "    model = create_enhanced_model(\n",
    "        input_shape=(hp['window_size'], data.shape[1]),\n",
    "        lstm_units1=hp['lstm_units1'],\n",
    "        lstm_units2=hp['lstm_units2'],\n",
    "        dropout_rate=hp['dropout_rate']\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp['learning_rate']),\n",
    "        loss={\n",
    "            'direction': lambda y_true, y_pred: asymmetric_focal_loss(\n",
    "                y_true, y_pred,\n",
    "                alpha=list(class_weights.values()),\n",
    "                gamma=hp['gamma']\n",
    "            ),\n",
    "            'high': 'huber',\n",
    "            'low': 'huber'\n",
    "        },\n",
    "        loss_weights=[hp['loss_weights_dir'], 0.5*(1-hp['loss_weights_dir']), 0.5*(1-hp['loss_weights_dir'])],\n",
    "        metrics={'direction': [DirectionAccuracy()]}\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=hp['batch_size'],\n",
    "        callbacks=[\n",
    "            TFKerasPruningCallback(trial, 'val_direction_accuracy'),\n",
    "            EarlyStopping(monitor='val_direction_accuracy', patience=10, mode='max')\n",
    "        ],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return max(history.history['val_direction_accuracy'])\n",
    "\n",
    "# 6. Основной запуск\n",
    "if __name__ == \"__main__\":\n",
    "    # Загружаем данные\n",
    "    data = df\n",
    "    scaler = RobustScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    # Балансировка классов\n",
    "    X, y = create_sequences(scaled_data, window_size=50)\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(np.argmax(y[0], axis=1)),\n",
    "        y=np.argmax(y[0], axis=1)\n",
    "    )\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    # Оптимизация\n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "    study.optimize(objective, n_trials=50, timeout=3600*3)\n",
    "    \n",
    "    print(f\"Лучшие параметры: {study.best_params}\")\n",
    "    print(f\"Лучший результат: {study.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 65ms/step - direction_accuracy: 0.1775 - direction_direction_accuracy: 0.1775 - direction_loss: -1.6364 - high_loss: 0.2408 - loss: -0.6511 - low_loss: 0.1699 - val_direction_accuracy: 0.1301 - val_direction_direction_accuracy: 0.1301 - val_direction_loss: -31.7305 - val_high_loss: 0.5981 - val_loss: -14.4295 - val_low_loss: 0.6549 - learning_rate: 1.8200e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 64ms/step - direction_accuracy: 0.1243 - direction_direction_accuracy: 0.1243 - direction_loss: -45.0160 - high_loss: 1.9639 - loss: -20.1411 - low_loss: 0.9948 - val_direction_accuracy: 0.1301 - val_direction_direction_accuracy: 0.1301 - val_direction_loss: -97.4840 - val_high_loss: 0.8520 - val_loss: -44.9292 - val_low_loss: 0.7405 - learning_rate: 1.8200e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m434/434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 70ms/step - direction_accuracy: 0.1247 - direction_direction_accuracy: 0.1247 - direction_loss: -110.6614 - high_loss: 0.6091 - loss: -51.2446 - low_loss: 0.1864 - val_direction_accuracy: 0.1301 - val_direction_direction_accuracy: 0.1301 - val_direction_loss: -173.9898 - val_high_loss: 0.8006 - val_loss: -80.5222 - val_low_loss: 0.7959 - learning_rate: 1.8200e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m154/434\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 64ms/step - direction_accuracy: 0.1243 - direction_direction_accuracy: 0.1243 - direction_loss: -176.6220 - high_loss: 0.1658 - loss: -82.0444 - low_loss: 0.1514"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 79\u001b[0m\n\u001b[0;32m     67\u001b[0m best_params \u001b[39m=\u001b[39m {\n\u001b[0;32m     68\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mwindow_size\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m57\u001b[39m,\n\u001b[0;32m     69\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlstm_units1\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m64\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mloss_weights_dir\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.465\u001b[39m\n\u001b[0;32m     76\u001b[0m }\n\u001b[0;32m     78\u001b[0m \u001b[39m# Запуск обучения\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m final_model, training_history \u001b[39m=\u001b[39m train_final_model(best_params)\n\u001b[0;32m     81\u001b[0m \u001b[39m# Визуализация результатов\u001b[39;00m\n\u001b[0;32m     82\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m14\u001b[39m,\u001b[39m5\u001b[39m))\n",
      "Cell \u001b[1;32mIn[12], line 43\u001b[0m, in \u001b[0;36mtrain_final_model\u001b[1;34m(best_params)\u001b[0m\n\u001b[0;32m     25\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m     26\u001b[0m     optimizer\u001b[39m=\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mbest_params[\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[0;32m     27\u001b[0m     loss\u001b[39m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m     metrics\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mdirection\u001b[39m\u001b[39m'\u001b[39m: [DirectionAccuracy(), \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     42\u001b[0m \u001b[39m# Обучение с расширенными callback'ами\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     44\u001b[0m     X_train, y_train,\n\u001b[0;32m     45\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_val, y_val),\n\u001b[0;32m     46\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[0;32m     47\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbest_params[\u001b[39m'\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     48\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[0;32m     49\u001b[0m         EarlyStopping(\n\u001b[0;32m     50\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_direction_direction_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     51\u001b[0m             patience\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[0;32m     52\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     53\u001b[0m             restore_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     54\u001b[0m         ),\n\u001b[0;32m     55\u001b[0m         ReduceLROnPlateau(\n\u001b[0;32m     56\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_direction_direction_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     57\u001b[0m             factor\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m,\n\u001b[0;32m     58\u001b[0m             patience\u001b[39m=\u001b[39;49m\u001b[39m7\u001b[39;49m,\n\u001b[0;32m     59\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m     ]\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m model, history\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39mfor\u001b[39;00m step, iterator \u001b[39min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    370\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 371\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m    372\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    373\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunction\u001b[39m(iterator):\n\u001b[0;32m    216\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    217\u001b[0m         iterator, (tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mIterator, tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mDistributedIterator)\n\u001b[0;32m    218\u001b[0m     ):\n\u001b[1;32m--> 219\u001b[0m         opt_outputs \u001b[39m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m    220\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_outputs\u001b[39m.\u001b[39mhas_value():\n\u001b[0;32m    221\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_flat(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    253\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1684\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1685\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1686\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1687\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1688\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1689\u001b[0m   )\n\u001b[0;32m   1690\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_final_model(best_params):\n",
    "    # Загрузка данных\n",
    "    data = df  # Заменить на реальные данные\n",
    "    scaler = RobustScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    # Создание последовательностей с лучшим window_size\n",
    "    X, y = create_sequences(scaled_data, best_params['window_size'])\n",
    "    \n",
    "    # Разделение данных\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, X_val = X[:split], X[split:]\n",
    "    y_train = [y[0][:split], y[1][:split], y[2][:split]]\n",
    "    y_val = [y[0][split:], y[1][split:], y[2][split:]]\n",
    "\n",
    "    # Создание модели с лучшими параметрами\n",
    "    model = create_enhanced_model(\n",
    "        input_shape=(best_params['window_size'], data.shape[1]),\n",
    "        lstm_units1=best_params['lstm_units1'],\n",
    "        lstm_units2=best_params['lstm_units2'],\n",
    "        dropout_rate=best_params['dropout_rate']\n",
    "    )\n",
    "\n",
    "    # Компиляция с оптимальными настройками\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=best_params['learning_rate']),\n",
    "        loss={\n",
    "            'direction': lambda y_true, y_pred: asymmetric_focal_loss(\n",
    "                y_true, y_pred,\n",
    "                alpha=list(class_weights.values()),\n",
    "                gamma=best_params['gamma']\n",
    "            ),\n",
    "            'high': 'huber',\n",
    "            'low': 'huber'\n",
    "        },\n",
    "        loss_weights=[best_params['loss_weights_dir'], \n",
    "                     0.5*(1-best_params['loss_weights_dir']), \n",
    "                     0.5*(1-best_params['loss_weights_dir'])],\n",
    "        metrics={'direction': [DirectionAccuracy(), 'accuracy']}\n",
    "    )\n",
    "\n",
    "    # Обучение с расширенными callback'ами\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=100,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor='val_direction_direction_accuracy',\n",
    "                patience=15,\n",
    "                mode='max',\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_direction_direction_accuracy',\n",
    "                factor=0.3,\n",
    "                patience=7,\n",
    "                mode='max'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Ваши лучшие параметры\n",
    "best_params = {\n",
    "    'window_size': 57,\n",
    "    'lstm_units1': 64,\n",
    "    'lstm_units2': 64,\n",
    "    'dropout_rate': 0.43,\n",
    "    'learning_rate': 0.000182,\n",
    "    'gamma': 2.756,\n",
    "    'batch_size': 64,\n",
    "    'loss_weights_dir': 0.465\n",
    "}\n",
    "\n",
    "# Запуск обучения\n",
    "final_model, training_history = train_final_model(best_params)\n",
    "\n",
    "# Визуализация результатов\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(training_history.history['direction_direction_accuracy'], label='Train')\n",
    "plt.plot(training_history.history['val_direction_direction_accuracy'], label='Validation')\n",
    "plt.title('Direction Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(training_history.history['loss'], label='Train Loss')\n",
    "plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Dynamics')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 57, 16), found shape=(None, 57, 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 172\u001b[0m\n\u001b[0;32m    169\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 172\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[17], line 132\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    113\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[0;32m    114\u001b[0m     optimizer\u001b[39m=\u001b[39mAdagrad(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.00018206566517784672\u001b[39m),\n\u001b[0;32m    115\u001b[0m     loss\u001b[39m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     }\n\u001b[0;32m    129\u001b[0m )\n\u001b[0;32m    131\u001b[0m \u001b[39m# В вызове fit() удаляем class_weight\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    133\u001b[0m     X_train, y_train,\n\u001b[0;32m    134\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_val, y_val),\n\u001b[0;32m    135\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[0;32m    136\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[0;32m    137\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[\n\u001b[0;32m    138\u001b[0m         EarlyStopping(\n\u001b[0;32m    139\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_direction_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    140\u001b[0m             patience\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m,\n\u001b[0;32m    141\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    142\u001b[0m             restore_best_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    143\u001b[0m         ),\n\u001b[0;32m    144\u001b[0m         ReduceLROnPlateau(\n\u001b[0;32m    145\u001b[0m             monitor\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval_direction_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m    146\u001b[0m             factor\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[0;32m    147\u001b[0m             patience\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m    148\u001b[0m             mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[0;32m    149\u001b[0m         )\n\u001b[0;32m    150\u001b[0m     ]\n\u001b[0;32m    151\u001b[0m )\n\u001b[0;32m    155\u001b[0m \u001b[39m# Визуализация\u001b[39;00m\n\u001b[0;32m    156\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m,\u001b[39m5\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39mif\u001b[39;00m spec_dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m dim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[39mif\u001b[39;00m spec_dim \u001b[39m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mincompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected shape=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfound shape=\u001b[39m\u001b[39m{\u001b[39;00mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"functional_3\" is incompatible with the layer: expected shape=(None, 57, 16), found shape=(None, 57, 14)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 1. Кастомная метрика для точности направления\n",
    "class DirectionAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='direction_accuracy', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.correct = self.add_weight(name='correct', initializer='zeros')\n",
    "        self.total = self.add_weight(name='total', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred_class = tf.argmax(y_pred, axis=1)\n",
    "        y_true_class = tf.argmax(y_true, axis=1)\n",
    "        matches = tf.cast(tf.equal(y_pred_class, y_true_class), tf.float32)\n",
    "        self.correct.assign_add(tf.reduce_sum(matches))\n",
    "        self.total.assign_add(tf.cast(tf.size(matches), tf.float32))\n",
    "\n",
    "    def result(self):\n",
    "        return self.correct / self.total\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.correct.assign(0.0)\n",
    "        self.total.assign(0.0)\n",
    "\n",
    "# 2. Улучшенная функция потерь\n",
    "def asymmetric_focal_loss(y_true, y_pred, alpha=[1.0, 1.0, 1.0], gamma=2.0):\n",
    "    ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    probs = tf.reduce_max(y_true * y_pred, axis=1)\n",
    "    alpha_weights = tf.reduce_sum(y_true * alpha, axis=1)\n",
    "    focal_loss = alpha_weights * (1 - probs)**gamma * ce_loss\n",
    "    return tf.reduce_mean(focal_loss)\n",
    "\n",
    "# 3. Генератор временных окон с аугментацией\n",
    "def create_sequences(data, window_size, step=1):\n",
    "    sequences = []\n",
    "    targets_dir = []\n",
    "    targets_high = []\n",
    "    targets_low = []\n",
    "    \n",
    "    for i in range(window_size, len(data)-1):\n",
    "        sequences.append(data[i-window_size:i])\n",
    "        targets_dir.append(data[i, 0:3])\n",
    "        targets_high.append(data[i+1, 3])  # Прогноз на следующий шаг\n",
    "        targets_low.append(data[i+1, 4])\n",
    "    \n",
    "    return (np.array(sequences),\n",
    "            [np.array(targets_dir),\n",
    "             np.array(targets_high).reshape(-1,1),\n",
    "             np.array(targets_low).reshape(-1,1)])\n",
    "\n",
    "# 4. Улучшенная архитектура модели\n",
    "def create_trading_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Ветвь временных зависимостей\n",
    "    x = LSTM(128, return_sequences=True, dropout=0.3)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LSTM(64, dropout=0.2)(x)\n",
    "    \n",
    "    # Ветвь технических индикаторов\n",
    "    tech = inputs[:, -1, -3:]  # Последние 3 технических индикатора\n",
    "    tech = Dense(16, activation='selu')(tech)\n",
    "    \n",
    "    # Объединение ветвей\n",
    "    merged = concatenate([x, tech])\n",
    "    merged = Dense(64, activation='selu')(merged)\n",
    "    merged = Dropout(0.3)(merged)\n",
    "    \n",
    "    # Выходы\n",
    "    direction = Dense(32, activation='selu')(merged)\n",
    "    direction = Dense(3, activation='softmax', name='direction')(direction)\n",
    "    \n",
    "    hl = Dense(32, activation='selu')(merged)\n",
    "    high = Dense(1, activation='linear', name='high')(hl)\n",
    "    low = Dense(1, activation='linear', name='low')(hl)\n",
    "    \n",
    "    return Model(inputs, [direction, high, low])\n",
    "\n",
    "# 5. Полный пайплайн\n",
    "def main():\n",
    "    # Загрузка и подготовка данных\n",
    "    data = df\n",
    "    scaler = RobustScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Создание последовательностей\n",
    "    window_size = 57\n",
    "    X, y = create_sequences(scaled_data, window_size)\n",
    "    \n",
    "    # Балансировка классов\n",
    "    classes = np.unique(np.argmax(y[0], axis=1))  # Автоматическое определение классов\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=classes,  # Должен быть numpy array\n",
    "        y=np.argmax(y[0], axis=1)\n",
    "    )\n",
    "    class_weights = dict(zip(classes, class_weights))\n",
    "    \n",
    "    # Разделение данных\n",
    "    split = int(0.8 * len(X))\n",
    "    X_train, X_val = X[:split], X[split:]\n",
    "    y_train, y_val = [y[0][:split], y[1][:split], y[2][:split]], \\\n",
    "                     [y[0][split:], y[1][split:], y[2][split:]]\n",
    "    \n",
    "    # Создание и компиляция модели\n",
    "    model = create_trading_model((window_size, 16))\n",
    "    model.compile(\n",
    "        optimizer=Adagrad(learning_rate=0.00018206566517784672),\n",
    "        loss={\n",
    "            'direction': lambda y_true, y_pred: asymmetric_focal_loss(\n",
    "                y_true, y_pred, \n",
    "                alpha=list(class_weights.values())  # Передаем веса как параметр\n",
    "            ),\n",
    "            'high': 'huber',\n",
    "            'low': 'huber'\n",
    "        },\n",
    "        loss_weights=[0.6, 0.2, 0.2],\n",
    "        metrics={\n",
    "            'direction': [DirectionAccuracy(), 'accuracy'],\n",
    "            'high': ['mae'],\n",
    "            'low': ['mae']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # В вызове fit() удаляем class_weight\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        callbacks=[\n",
    "            EarlyStopping(\n",
    "                monitor='val_direction_accuracy',\n",
    "                patience=12,\n",
    "                mode='max',\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_direction_accuracy',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                mode='max'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    # Визуализация\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['direction_accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_direction_accuracy'], label='Val Accuracy')\n",
    "    plt.axhline(0.5, color='red', linestyle='--', label='50% Threshold')\n",
    "    plt.title('Direction Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Total Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - direction_accuracy: 0.3459 - direction_loss: 0.0056 - high_loss: 0.5104 - high_mae: 0.5105 - loss: 0.1604 - low_loss: 0.5323 - low_mae: 0.5325  "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 145\n'y' sizes: 145, 145, 291\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[291], line 146\u001b[0m\n\u001b[0;32m    139\u001b[0m val_targets \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdirection\u001b[39m\u001b[39m\"\u001b[39m: y_dir_val, \u001b[39m\"\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m\"\u001b[39m: y_high_val, \u001b[39m\"\u001b[39m\u001b[39mlow\u001b[39m\u001b[39m\"\u001b[39m: y_low_val}\n\u001b[0;32m    141\u001b[0m callbacks \u001b[39m=\u001b[39m [\n\u001b[0;32m    142\u001b[0m     ReduceLROnPlateau(monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m    143\u001b[0m     EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_direction_accuracy\u001b[39m\u001b[39m\"\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, restore_best_weights\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    144\u001b[0m ]\n\u001b[1;32m--> 146\u001b[0m history_stage1 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    147\u001b[0m     X_train,\n\u001b[0;32m    148\u001b[0m     train_targets,\n\u001b[0;32m    149\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m    150\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,\n\u001b[0;32m    151\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(X_val, val_targets),\n\u001b[0;32m    152\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    153\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m    154\u001b[0m )\n\u001b[0;32m    156\u001b[0m \u001b[39m# -------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m# 4. Оценка модели\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39m# -------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mModel Evaluation:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\79112\\.conda\\envs\\myenv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\data_adapter_utils.py:115\u001b[0m, in \u001b[0;36mcheck_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    111\u001b[0m     sizes \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m    112\u001b[0m         \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tree\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m sizes: \u001b[39m\u001b[39m{\u001b[39;00msizes\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 115\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous. Make sure all arrays contain the same number of samples.'x' sizes: 145\n'y' sizes: 145, 145, 291\n"
     ]
    }
   ],
   "source": [
    "def scale_data(dummy_data):\n",
    "    \"\"\"Масштабирование данных.\"\"\"\n",
    "    scaler = RobustScaler()\n",
    "    return scaler.fit_transform(dummy_data)\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Динамический график изменения learning rate с warm-up.\"\"\"\n",
    "    def __init__(self, initial_lr=1e-4, warmup_steps=1000):\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        return tf.minimum(self.initial_lr * tf.sqrt(step / self.warmup_steps), self.initial_lr)\n",
    "\n",
    "def weighted_asymmetric_loss(y_true, y_pred):\n",
    "    \"\"\"Асимметричная взвешенная cross-entropy loss.\"\"\"\n",
    "    true_idx = tf.argmax(y_true, axis=1)\n",
    "    weights = tf.where(true_idx == 2, 1.5, tf.where(true_idx == 1, 0.7, 1.0))\n",
    "    ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return tf.reduce_mean(ce * weights)\n",
    "\n",
    "def mare_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Absolute Relative Error (MARE).\"\"\"\n",
    "    return tf.reduce_mean(tf.abs((y_true - y_pred) / (y_true + 1e-8)))\n",
    "\n",
    "class CustomLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Комбинированная функция потерь.\"\"\"\n",
    "    def __init__(self, alpha=0.3, beta=0.2, name=\"custom_loss\"):\n",
    "        super().__init__(name=name)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        direction_true, high_true, low_true = y_true\n",
    "        direction_pred, high_pred, low_pred = y_pred\n",
    "\n",
    "        ce_loss = weighted_asymmetric_loss(direction_true, direction_pred)\n",
    "        hl_loss = 0.5 * (mare_loss(high_true, high_pred) + mare_loss(low_true, low_pred))\n",
    "        return ce_loss + self.alpha * hl_loss\n",
    "\n",
    "def prepare_data(data, window_size=30):\n",
    "    X, y_dir, y_high, y_low = [], [], [], []\n",
    "    for i in range(window_size, len(data)):\n",
    "        X.append(data[i - window_size : i])\n",
    "        y_dir.append(data[i, 0:3])\n",
    "        y_high.append(data[i, 3])  # High\n",
    "        y_low.append(data[i, 4])   # Low\n",
    "\n",
    "    # Возвращаем данные ПОСЛЕ завершения цикла\n",
    "    X = np.array(X)\n",
    "    y_dir = np.array(y_dir)\n",
    "    y_high = np.array(y_high).reshape(-1, 1)\n",
    "    y_low = np.array(y_low).reshape(-1, 1)\n",
    "    \n",
    "    return X, [y_dir, y_high, y_low]\n",
    "\n",
    "def create_lstm_model(input_shape, n_units=64, freeze_encoder=False):\n",
    "    \"\"\"Создание мультивыходной LSTM-модели.\"\"\"\n",
    "    inputs = Input(shape=input_shape, name=\"input_layer\")\n",
    "    x = LSTM(n_units, return_sequences=True, name=\"lstm\")(inputs)\n",
    "\n",
    "    # Направление\n",
    "    direction_branch = LSTM(n_units // 2, return_sequences=False, name=\"lstm2\")(x)\n",
    "    direction_out = Dense(3, activation=\"softmax\", name=\"direction\")(direction_branch)\n",
    "\n",
    "    # Прогноз High/Low\n",
    "    atr = Lambda(lambda x: x[:, :, -1:], name=\"atr_slice\")(inputs)  # Последний столбец — ATR\n",
    "    atr = Reshape((input_shape[0], 1), name=\"atr_reshape\")(atr)     # Приводим к совместимой форме\n",
    "    hl_concat = concatenate([x, atr], axis=-1, name=\"concat_atr\")\n",
    "\n",
    "    hl_hidden = Dense(32, activation=\"selu\", name=\"hl_dense\")(hl_concat)\n",
    "    high_out = Dense(1, activation=\"linear\", name=\"high\")(hl_hidden)\n",
    "    low_out = Dense(1, activation=\"linear\", name=\"low\")(hl_hidden)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[direction_out, high_out, low_out])\n",
    "    if freeze_encoder:\n",
    "        for layer in model.layers[:-6]:\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. Подготовка данных\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Сгенерируем данные для примера\n",
    "np.random.seed(42)  # Для воспроизводимости\n",
    "dummy_data = np.random.rand(1000, 16)  # Пример ваших данных (1000 наблюдений, 16 признаков)\n",
    "\n",
    "# Масштабирование\n",
    "scaled_data = scale_data(dummy_data)\n",
    "\n",
    "# Создание временных окон\n",
    "window_size = 30\n",
    "X_seq, y_seq = prepare_data(scaled_data, window_size=window_size)\n",
    "\n",
    "# Разделение данных\n",
    "total_samples = X_seq.shape[0]\n",
    "train_end = int(0.7 * total_samples)\n",
    "val_end = int(0.85 * total_samples)\n",
    "\n",
    "X_train = X_seq[:train_end]\n",
    "X_val = X_seq[train_end:val_end]\n",
    "X_test = X_seq[val_end:]\n",
    "\n",
    "y_dir_train, y_high_train, y_low_train = y_seq[0][:train_end], y_seq[1][:train_end], y_seq[2][:train_end]\n",
    "y_dir_val,   y_high_val,   y_low_val   = y_seq[0][train_end:val_end], y_seq[1][train_end:val_end], y_seq[2][train_end:]\n",
    "y_dir_test,  y_high_test,  y_low_test  = y_seq[0][val_end:], y_seq[1][val_end:], y_seq[2][val_end:]\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 3. Создание и обучение модели\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Создание модели\n",
    "model = create_lstm_model(input_shape=(window_size, 16), n_units=64)\n",
    "\n",
    "# Компиляция модели\n",
    "lr_schedule = CustomSchedule()\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss={\n",
    "        \"direction\": weighted_asymmetric_loss,\n",
    "        \"high\": \"mae\",\n",
    "        \"low\": \"mae\"\n",
    "    },\n",
    "    loss_weights=[0.7, 0.15, 0.15],\n",
    "    metrics={\n",
    "        \"direction\": \"accuracy\",\n",
    "        \"high\": \"mae\",\n",
    "        \"low\": \"mae\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "train_targets = {\"direction\": y_dir_train, \"high\": y_high_train, \"low\": y_low_train}\n",
    "val_targets = {\"direction\": y_dir_val, \"high\": y_high_val, \"low\": y_low_val}\n",
    "\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, verbose=1),\n",
    "    EarlyStopping(monitor=\"val_direction_accuracy\", patience=10, verbose=1, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history_stage1 = model.fit(\n",
    "    X_train,\n",
    "    train_targets,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, val_targets),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 4. Оценка модели\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "test_results = model.evaluate(X_test, {\"direction\": y_dir_test, \"high\": y_high_test, \"low\": y_low_test})\n",
    "print(\"Test Results:\", test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
